{"question": "What is the Central Limit Theorem and why is it important?", "answer": "The Central Limit Theorem (CLT) states that the distribution of the sample mean approximates a normal distribution as the sample size becomes large, regardless of the population's distribution, provided the samples are independent and identically distributed. It’s important because it allows us to make inferences about population parameters using sample statistics, enabling hypothesis testing and confidence intervals even for non-normal populations.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is sampling? How many sampling methods do you know?", "answer": "Sampling is a statistical technique used to select a representative subset of data points from a larger population to analyze patterns and trends. Common sampling methods include: 1. Simple Random Sampling (every member has an equal chance), 2. Stratified Sampling (population divided into strata), 3. Cluster Sampling (groups or clusters are randomly selected), 4. Systematic Sampling (every nth member is selected), 5. Convenience Sampling (non-random, based on accessibility).", "difficulty": "easy", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the difference between type I vs type II error?", "answer": "A Type I error occurs when the null hypothesis is true but is rejected (false positive). A Type II error occurs when the null hypothesis is false but fails to be rejected (false negative). Type I errors are often considered more serious in hypothesis testing, as they lead to incorrect rejections of a true null hypothesis.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is linear regression? What do the terms p-value, coefficient, and r-squared value mean?", "answer": "Linear regression is a statistical method to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation. The p-value indicates the probability that a coefficient is zero (no effect), with p < 0.05 suggesting significance. Coefficients represent the change in the dependent variable per unit change in the independent variable. R-squared measures the proportion of variance in the dependent variable explained by the model, ranging from 0 to 1.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are the assumptions required for linear regression?", "answer": "Linear regression assumes: 1. Linear relationship between dependent and independent variables. 2. Errors are normally distributed and independent. 3. Minimal multicollinearity among independent variables. 4. Homoscedasticity (constant variance of errors across all levels of the independent variables).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a statistical interaction?", "answer": "A statistical interaction occurs when the effect of one independent variable on the dependent variable differs depending on the level of another independent variable. For example, the effect of exercise on weight loss may depend on diet.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is selection bias?", "answer": "Selection bias occurs when the sample data is not representative of the population due to non-random selection, leading to skewed results. For example, surveying only urban residents for a national study introduces selection bias.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is an example of a data set with a non-Gaussian distribution?", "answer": "An example is income data, which is often right-skewed (exponential or log-normal distribution) due to a few high earners and many lower earners. Other examples include time-to-failure data or count data (Poisson distribution).", "difficulty": "easy", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the Binomial Probability Formula?", "answer": "The binomial probability formula is P(X = k) = C(n, k) * π^k * (1-π)^(n-k), where n is the number of trials, k is the number of successes, π is the probability of success, and C(n, k) is the binomial coefficient.", "difficulty": "medium", "topic": "Probability", "role": "Data Scientist"}
{"question": "What is Data Science? List the differences between supervised and unsupervised learning.", "answer": "Data Science is an interdisciplinary field that uses tools, algorithms, and machine learning to extract insights from data. Differences: Supervised learning uses labeled data for prediction (e.g., classification, regression), while unsupervised learning uses unlabeled data for analysis (e.g., clustering, dimensionality reduction). Supervised learning has a training phase; unsupervised does not.", "difficulty": "easy", "topic": "Data Science", "role": "Data Scientist"}
{"question": "In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the probability that you see at least one shooting star in the period of an hour?", "answer": "Probability of not seeing a shooting star in 15 minutes = 1 - 0.2 = 0.8. In one hour (four 15-minute intervals), probability of not seeing any = 0.8^4 = 0.4096. Probability of seeing at least one = 1 - 0.4096 = 0.5904.", "difficulty": "medium", "topic": "Probability", "role": "Data Scientist"}
{"question": "A jar has 1000 coins, of which 999 are fair and 1 is double-headed. Pick a coin at random, and toss it 10 times. Given that you see 10 heads, what is the probability that the next toss of that coin is also a head?", "answer": "Let A be the event of picking a fair coin, and B be the event of picking the double-headed coin. P(A) = 0.999, P(B) = 0.001. P(10 heads | A) = (1/2)^10 = 1/1024, P(10 heads | B) = 1. Using Bayes’ theorem: P(B | 10 heads) = [P(10 heads | B) * P(B)] / [P(10 heads | A) * P(A) + P(10 heads | B) * P(B)] = [1 * 0.001] / [(1/1024) * 0.999 + 1 * 0.001] ≈ 0.5061. P(A | 10 heads) = 1 - 0.5061 = 0.4939. Probability of heads on next toss = P(A | 10 heads) * 0.5 + P(B | 10 heads) * 1 = 0.4939 * 0.5 + 0.5061 * 1 ≈ 0.7531.", "difficulty": "hard", "topic": "Probability", "role": "Data Scientist"}
{"question": "What is the difference between 'long' and 'wide' format data?", "answer": "In wide format, a subject’s repeated responses are in a single row, with each response in a separate column. In long format, each row represents a single time point per subject. Wide format is compact for analysis, while long format is better for longitudinal studies.", "difficulty": "easy", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What do you understand by the term Normal Distribution?", "answer": "A Normal Distribution is a symmetric, bell-shaped probability distribution characterized by its mean and standard deviation. Properties include: unimodal, symmetric, mean = median = mode, and asymptotic tails. It’s widely used in statistics due to the Central Limit Theorem.", "difficulty": "easy", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is correlation and covariance in statistics?", "answer": "Covariance measures how two variables change together, with positive values indicating they increase together and negative values indicating an inverse relationship. Correlation is a standardized measure of covariance, ranging from -1 to 1, indicating the strength and direction of the linear relationship.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the difference between Point Estimates and Confidence Interval?", "answer": "A point estimate is a single value estimating a population parameter (e.g., sample mean). A confidence interval is a range of values likely to contain the parameter, with a confidence level (e.g., 95%) indicating the probability the interval contains the true value.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the goal of A/B Testing?", "answer": "A/B Testing is a statistical method to compare two versions (A and B) of a variable (e.g., webpage design) to determine which performs better. The goal is to maximize an outcome of interest, such as click-through rates, by identifying significant differences.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is p-value?", "answer": "The p-value is the probability of observing results at least as extreme as those in the sample, assuming the null hypothesis is true. A low p-value (≤ 0.05) suggests evidence against the null hypothesis, while a high p-value (≥ 0.05) supports it.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "How can you generate a random number between 1-7 with only a die?", "answer": "Roll a six-sided die twice to get 36 outcomes. Exclude (6,6) and divide the remaining 35 outcomes into 7 groups of 5 (e.g., (1,1) to (1,5) for 1, (2,1) to (2,5) for 2, etc.). If (6,6) is rolled, repeat. Each group has an equal probability of 5/36.", "difficulty": "hard", "topic": "Probability", "role": "Data Scientist"}
{"question": "A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?", "answer": "Possible outcomes for two children are BB, BG, GB, GG. Given at least one is a girl, we exclude BB, leaving BG, GB, GG. The probability of GG (two girls) is 1/3.", "difficulty": "medium", "topic": "Probability", "role": "Data Scientist"}
{"question": "What do you understand by statistical power of sensitivity and how do you calculate it?", "answer": "Sensitivity (or statistical power) measures the proportion of true positives correctly identified by a classifier. It’s calculated as: Sensitivity = True Positives / (True Positives + False Negatives). High sensitivity indicates effective detection of positive cases.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Why is re-sampling done?", "answer": "Resampling is used to: 1. Estimate the accuracy of sample statistics (e.g., bootstrapping). 2. Perform significance tests by substituting labels. 3. Validate models using random subsets (e.g., cross-validation). It helps assess model robustness and generalization.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What are the differences between over-fitting and under-fitting?", "answer": "Overfitting occurs when a model learns noise in the training data, leading to poor generalization (high variance). Underfitting occurs when a model is too simple to capture the data’s patterns, resulting in high bias. Both lead to poor predictive performance.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How to combat overfitting and underfitting?", "answer": "To combat overfitting: use cross-validation, simplify the model, apply regularization (e.g., L1, L2), or collect more data. To combat underfitting: increase model complexity, add features, or reduce regularization. Validation datasets help evaluate effectiveness.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is regularisation? Why is it useful?", "answer": "Regularization adds a penalty term to the loss function to prevent overfitting by constraining model complexity. Common methods include L1 (Lasso) and L2 (Ridge). It’s useful for improving generalization and handling multicollinearity.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the Law of Large Numbers?", "answer": "The Law of Large Numbers states that as the sample size increases, the sample mean, variance, and standard deviation converge to the population parameters, assuming independent and identically distributed samples. It underpins statistical inference.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What are confounding variables?", "answer": "Confounding variables are extraneous factors that correlate with both the dependent and independent variables, potentially leading to biased conclusions. For example, age may confound the relationship between exercise and weight gain.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What are the types of biases that can occur during sampling?", "answer": "Types of sampling biases include: 1. Selection bias (non-random sample). 2. Undercoverage bias (some population segments are excluded). 3. Survivorship bias (focusing only on surviving entities).", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is survivorship bias?", "answer": "Survivorship bias is the error of focusing on entities that survived a process while ignoring those that did not, leading to skewed conclusions. For example, analyzing only successful companies may overlook factors causing failures.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "Explain how a ROC curve works?", "answer": "A ROC (Receiver Operating Characteristic) curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various classification thresholds. It evaluates a classifier’s performance, with the area under the curve (AUC) indicating discriminative ability (higher AUC = better model).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is TF/IDF vectorization?", "answer": "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects a word’s importance in a document relative to a corpus. TF measures word frequency in a document, while IDF reduces weight for common words. It’s used in text mining and information retrieval.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "Why do we generally use the Softmax non-linearity function as the last operation in a network?", "answer": "The Softmax function converts a vector of real numbers into a probability distribution, where each element is non-negative and sums to 1. It’s used in the final layer of classification networks to output probabilities for each class, enabling interpretable predictions.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "Python or R – Which one would you prefer for text analytics?", "answer": "Python is preferred for text analytics due to its Pandas library for data manipulation, NLTK and spaCy for text processing, and faster performance. R is better suited for statistical modeling and machine learning but is less efficient for text analytics.", "difficulty": "easy", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "How does data cleaning play a vital role in analysis?", "answer": "Data cleaning transforms raw data into a usable format, increases model accuracy by removing errors, and handles missing values or outliers. It’s critical as it can consume up to 80% of analysis time, ensuring reliable insights.", "difficulty": "easy", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "Differentiate between univariate, bivariate, and multivariate analysis.", "answer": "Univariate analysis examines one variable (e.g., mean of sales). Bivariate analysis studies the relationship between two variables (e.g., sales vs. temperature). Multivariate analysis involves three or more variables to understand complex relationships (e.g., house price vs. rooms, area, floors).", "difficulty": "easy", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "Explain Star Schema.", "answer": "A Star Schema is a database schema with a central fact table containing metrics and surrounding dimension tables (lookup tables) storing descriptive attributes. It’s efficient for real-time applications and data warehousing due to its simplicity and reduced memory usage.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What is Cluster Sampling?", "answer": "Cluster sampling involves dividing a population into clusters (e.g., cities) and randomly selecting entire clusters for analysis. It’s used when the population is geographically dispersed, reducing costs compared to simple random sampling.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is Systematic Sampling?", "answer": "Systematic sampling selects every nth element from an ordered list after a random starting point. It’s simple and ensures even coverage, often used when a complete population list is available.", "difficulty": "easy", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What are Eigenvectors and Eigenvalues?", "answer": "Eigenvalues are scalars indicating the magnitude of transformation along eigenvectors, which are vectors that remain in the same direction after a linear transformation. They’re used in data analysis (e.g., PCA) to identify principal components.", "difficulty": "hard", "topic": "Statistics", "role": "Data Scientist"}
{"question": "Can you cite some examples where a false positive is more important than a false negative?", "answer": "1. Medical testing: A false positive for cancer leads to unnecessary treatment, harming healthy patients. 2. E-commerce: Incorrectly giving vouchers to non-qualifying customers results in financial loss.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Can you cite some examples where a false negative is more important than a false positive?", "answer": "1. Airport security: A false negative (missing a threat) endangers safety. 2. Judicial system: Freeing a guilty criminal (false negative) has severe consequences. 3. Marriage prediction: Rejecting a good match (false negative) leads to missed opportunities.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Can you cite some examples where both false positive and false negatives are equally important?", "answer": "In banking, false positives (denying good loan applicants) lose customers, while false negatives (approving bad applicants) risk losses. Both impact profitability and require balanced model tuning.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Can you explain the difference between a Validation Set and a Test Set?", "answer": "A validation set is a subset of the training data used to tune model parameters and prevent overfitting. A test set is a separate dataset used to evaluate the final model’s performance, assessing its generalization to new data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Explain cross-validation.", "answer": "Cross-validation is a technique to assess a model’s performance by splitting data into training and validation sets multiple times (e.g., k-fold cross-validation). It reduces overfitting and provides a robust estimate of model generalization.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Machine Learning?", "answer": "Machine Learning is a field of computer science that enables computers to learn from data without explicit programming. It includes supervised learning (prediction), unsupervised learning (pattern discovery), and reinforcement learning (decision-making).", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Supervised Learning?", "answer": "Supervised learning infers a function from labeled training data to predict outcomes for new data. Algorithms include SVM, regression, decision trees, and neural networks. Example: Classifying fruits based on labeled examples.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Unsupervised Learning?", "answer": "Unsupervised learning draws inferences from unlabeled data, identifying patterns or structures. Algorithms include clustering, anomaly detection, and dimensionality reduction. Example: Grouping fruits by physical characteristics without labels.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is 'Naive' in a Naive Bayes?", "answer": "The 'naive' assumption in Naive Bayes is that all features are conditionally independent given the class label, which simplifies probability calculations but may not always hold true.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Explain SVM algorithm in detail.", "answer": "Support Vector Machines (SVM) is a supervised learning algorithm for classification and regression. It finds the optimal hyperplane that maximizes the margin between classes in an n-dimensional space. Support vectors are the closest points to the hyperplane. Kernels (e.g., linear, polynomial, RBF) handle non-linear data by transforming it into higher dimensions.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are the support vectors in SVM?", "answer": "Support vectors are the data points closest to the decision boundary (hyperplane) in an SVM model. They define the margin and are critical for determining the optimal hyperplane.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are the different kernels in SVM?", "answer": "SVM kernels include: 1. Linear (for linearly separable data). 2. Polynomial (captures polynomial relationships). 3. Radial Basis Function (RBF, for non-linear data). 4. Sigmoid (similar to neural networks).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Explain Decision Tree algorithm in detail.", "answer": "A Decision Tree is a supervised learning algorithm for classification and regression. It splits data into subsets based on feature values, creating a tree of decisions (nodes) and outcomes (leaves). Information gain or Gini impurity guides splits. Pruning reduces overfitting.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is pruning in Decision Tree?", "answer": "Pruning removes branches of a decision tree that have little predictive power, reducing model complexity and preventing overfitting. It can be pre-pruning (stopping early) or post-pruning (removing branches after building).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is logistic regression? State an example when you have used logistic regression recently.", "answer": "Logistic regression predicts a binary outcome using a linear combination of predictors transformed by the sigmoid function. Example: Predicting whether a customer will churn (yes/no) based on usage patterns.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Linear Regression?", "answer": "Linear regression models the relationship between a dependent variable and one or more independent variables using a linear equation. It predicts continuous outcomes, minimizing the sum of squared errors.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are the drawbacks of the linear model?", "answer": "Drawbacks include: 1. Assumes linearity of errors. 2. Unsuitable for count or binary outcomes. 3. Prone to overfitting with high-dimensional data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between Regression and classification ML techniques?", "answer": "Regression predicts continuous outcomes (e.g., house prices), while classification predicts discrete categories (e.g., spam/not spam). Both are supervised learning but differ in output type.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are Recommender Systems?", "answer": "Recommender systems predict user preferences for products using collaborative filtering (based on user behavior) or content-based filtering (based on item features). Examples: Netflix movie recommendations, Amazon product suggestions.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Collaborative filtering?", "answer": "Collaborative filtering predicts user preferences based on patterns from other users’ behaviors or ratings, without requiring item features. Example: Recommending movies based on similar users’ ratings.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How can outlier values be treated?", "answer": "Outliers can be: 1. Removed if they’re errors (e.g., invalid data). 2. Capped at percentiles (e.g., 1st or 99th). 3. Handled by robust algorithms (e.g., random forests). 4. Transformed (e.g., log transformation).", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What are the various steps involved in an analytics project?", "answer": "Steps include: 1. Define the business problem. 2. Explore and understand data. 3. Prepare data (handle missing values, outliers). 4. Model data and iterate. 5. Validate model on new data. 6. Deploy and monitor performance.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "During analysis, how do you treat missing values?", "answer": "Missing values can be: 1. Imputed with mean/median/mode. 2. Removed if few or non-critical. 3. Replaced with a default value. 4. Predicted using models. If >80% are missing, consider dropping the variable.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "How will you define the number of clusters in a clustering algorithm?", "answer": "Use the elbow method, where the within-cluster sum of squares (WSS) is plotted against the number of clusters (k). The ‘elbow’ point, where WSS decreases slowly, indicates the optimal k.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What cross-validation technique would you use on a time series data set?", "answer": "For time series, use forward-chaining cross-validation (e.g., train on [1], test on [2]; train on [1,2], test on [3]). This respects chronological order, unlike k-fold cross-validation.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a Box-Cox Transformation?", "answer": "A Box-Cox transformation transforms non-normal dependent variables into a normal shape using a power function, enabling the use of statistical methods that assume normality. It’s parameterized by lambda, chosen to maximize normality.", "difficulty": "hard", "topic": "Statistics", "role": "Data Scientist"}
{"question": "How regularly must an algorithm be updated?", "answer": "Update algorithms when: 1. Data patterns change (non-stationarity). 2. New data streams in. 3. Performance degrades. Regular monitoring ensures models remain accurate.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "If you are having 4GB RAM in your machine and you want to train your model on 10GB data set, how would you go about this problem?", "answer": "For neural networks, use small batch sizes with NumPy arrays, loading data incrementally. For SVM, use partial_fit with data subsets. Alternatively, use cloud platforms or optimize data preprocessing to reduce memory usage.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What do you mean by Deep Learning?", "answer": "Deep Learning is a subset of machine learning using artificial neural networks with multiple layers to model complex patterns, inspired by the human brain. It excels in tasks like image and speech recognition.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between machine learning and deep learning?", "answer": "Machine learning involves algorithms learning from data, including supervised, unsupervised, and reinforcement learning. Deep learning is a subset using deep neural networks to model complex patterns, requiring more data and computational power.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is reinforcement learning?", "answer": "Reinforcement learning involves an agent learning to make decisions by maximizing a cumulative reward through trial and error, guided by a reward/penalty system. Example: Training a robot to navigate a maze.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are Artificial Neural Networks?", "answer": "Artificial Neural Networks (ANNs) are algorithms inspired by biological neural networks, consisting of interconnected nodes (neurons) organized in layers. They learn by adjusting weights to minimize error, used for tasks like classification and regression.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "How are weights initialized in a network?", "answer": "Weights are typically initialized randomly near zero to ensure neurons perform different computations, avoiding symmetry. Initializing to zero makes the network behave like a linear model, reducing effectiveness.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the Cost Function?", "answer": "The cost function (loss function) measures the error between a model’s predictions and actual outcomes. It’s minimized during training via optimization (e.g., gradient descent) to improve model performance.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What are Hyperparameters?", "answer": "Hyperparameters are model settings defined before training, such as learning rate, number of hidden layers, or batch size. They control the training process and model architecture.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What will happen if the learning rate is set inaccurately (too low or too high)?", "answer": "A too-low learning rate slows training, requiring many iterations to converge. A too-high learning rate causes unstable updates, potentially leading to divergence or poor model performance.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between Epoch, Batch, and Iteration in Deep Learning?", "answer": "An epoch is one pass through the entire dataset. A batch is a subset of the dataset used in one training iteration. An iteration is one update of the model’s weights, equal to dataset size divided by batch size per epoch.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What are the different layers in CNN?", "answer": "CNN layers include: 1. Convolutional Layer (extracts features via filters). 2. ReLU Layer (adds non-linearity). 3. Pooling Layer (reduces spatial dimensions). 4. Fully Connected Layer (classifies features).", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is pooling in CNN, and how does it work?", "answer": "Pooling reduces spatial dimensions in a CNN by down-sampling feature maps (e.g., max pooling takes the maximum value in a region). It reduces computational load and prevents overfitting.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What are Recurrent Neural Networks (RNNs)?", "answer": "RNNs are neural networks designed for sequential data, where outputs from one step are fed as inputs to the next. They’re used for tasks like time series analysis or speech recognition but can suffer from vanishing gradients.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "How does an LSTM network work?", "answer": "Long Short-Term Memory (LSTM) networks are RNNs that handle long-term dependencies. They use gates (forget, input, output) to selectively remember or forget information, enabling better learning of sequential patterns.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is a Multi-layer Perceptron (MLP)?", "answer": "An MLP is a fully connected neural network with an input layer, one or more hidden layers, and an output layer. It uses non-linear activation functions and backpropagation to learn complex patterns.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "Explain Gradient Descent.", "answer": "Gradient Descent is an optimization algorithm that minimizes a cost function by iteratively updating model parameters in the direction of the negative gradient. Variants include stochastic, batch, and mini-batch gradient descent.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is exploding gradients?", "answer": "Exploding gradients occur in RNNs when gradients grow exponentially during backpropagation, causing unstable training and large weight updates. Solutions include gradient clipping or normalization.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is vanishing gradients?", "answer": "Vanishing gradients occur in RNNs when gradients become too small during backpropagation, slowing or stopping learning. Solutions include LSTMs, GRUs, or using ReLU activation.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is Back Propagation and explain its working?", "answer": "Backpropagation computes gradients of the loss function with respect to weights by propagating errors backward through the network. Steps: 1. Forward pass to compute predictions. 2. Calculate loss. 3. Backward pass to compute gradients. 4. Update weights using gradient descent.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What are the variants of Back Propagation?", "answer": "Variants include: 1. Stochastic Gradient Descent (SGD, updates per sample). 2. Batch Gradient Descent (updates per entire dataset). 3. Mini-batch Gradient Descent (updates per small batch, balances speed and stability).", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What are the different Deep Learning Frameworks?", "answer": "Common frameworks include: PyTorch, TensorFlow, Keras, Microsoft Cognitive Toolkit, Caffe, Chainer. Each offers tools for building and training neural networks.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the role of the Activation Function?", "answer": "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Common functions include ReLU, sigmoid, and tanh.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "Name a few Machine Learning libraries for various purposes.", "answer": "Libraries: 1. NumPy (scientific computation). 2. Pandas (tabular data). 3. Scikit-learn (modeling, preprocessing). 4. Statsmodels (time-series). 5. NLTK (text processing). 6. TensorFlow, PyTorch (deep learning).", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is an Auto-Encoder?", "answer": "An auto-encoder is a neural network that learns to reconstruct its input by encoding it into a lower-dimensional representation and decoding it. It’s used for dimensionality reduction and anomaly detection.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is a Boltzmann Machine?", "answer": "A Boltzmann Machine is a neural network with interconnected nodes that learns complex patterns via a probabilistic model. Restricted Boltzmann Machines (RBMs) have a single layer, making them faster for feature learning.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is Dropout and Batch Normalization?", "answer": "Dropout randomly deactivates neurons during training to prevent overfitting. Batch normalization normalizes layer inputs to have zero mean and unit variance, improving training stability and speed.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between Batch Gradient Descent and Stochastic Gradient Descent?", "answer": "Batch Gradient Descent computes gradients using the entire dataset, converging slowly but stably. Stochastic Gradient Descent uses one sample, converging faster but with more noise.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "Why is TensorFlow the most preferred library in Deep Learning?", "answer": "TensorFlow is preferred for its C++ and Python APIs, fast compilation, and support for both CPU and GPU. It’s flexible for building complex models and widely adopted.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What do you mean by Tensor in TensorFlow?", "answer": "A tensor is a multi-dimensional array representing data in TensorFlow, used as inputs and outputs in computational graphs. It generalizes scalars, vectors, and matrices.", "difficulty": "easy", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the Computational Graph?", "answer": "A computational graph in TensorFlow is a network of nodes (operations) and edges (tensors) representing the flow of data. It enables efficient computation and optimization of neural networks.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "How is logistic regression done?", "answer": "Logistic regression predicts a binary outcome by modeling the relationship between features and the log-odds of the outcome, using the sigmoid function to output probabilities between 0 and 1.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Explain the steps in making a decision tree.", "answer": "1. Take the entire dataset as input. 2. Calculate entropy or Gini impurity for the target. 3. Compute information gain for each feature. 4. Choose the feature with the highest gain as the root. 5. Repeat for each branch until a stopping criterion is met. 6. Prune to reduce overfitting.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you build a random forest model?", "answer": "1. Select k random features from m total features. 2. Build a decision tree on a bootstrapped sample, choosing the best split. 3. Repeat to create n trees. 4. Aggregate predictions (majority vote or average).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How can you avoid overfitting your model?", "answer": "1. Simplify the model (fewer parameters). 2. Use cross-validation (e.g., k-fold). 3. Apply regularization (e.g., L1, L2). 4. Collect more data. 5. Use dropout in neural networks.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What are the feature selection methods used to select the right variables?", "answer": "Filter methods: Linear discriminant analysis, ANOVA, Chi-Square. Wrapper methods: Forward selection, backward elimination, recursive feature elimination. Filter methods are faster, while wrapper methods are more accurate but computationally intensive.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "In your choice of language, write a program that prints the numbers ranging from one to 50. But for multiples of three, print 'Fizz' instead of the number and for the multiples of five, print 'Buzz'. For numbers which are multiples of both three and five, print 'FizzBuzz'.", "answer": "Python code: for i in range(1, 51): if i % 3 == 0 and i % 5 == 0: print('FizzBuzz'); elif i % 3 == 0: print('Fizz'); elif i % 5 == 0: print('Buzz'); else: print(i)", "difficulty": "easy", "topic": "Coding", "role": "Data Scientist"}
{"question": "You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?", "answer": "For large datasets, remove rows with missing values. For smaller datasets, impute missing values with mean, median, or mode using pandas (e.g., df.fillna(df.mean())). Alternatively, use predictive models to estimate missing values.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "For the given points, how will you calculate the Euclidean distance in Python? plot1 = [1,3], plot2 = [2,5]", "answer": "Python code: euclidean_distance = ((plot1[0] - plot2[0])**2 + (plot1[1] - plot2[1])**2)**0.5. For plot1=[1,3], plot2=[2,5], distance = sqrt((1-2)^2 + (3-5)^2) = sqrt(5) ≈ 2.236.", "difficulty": "easy", "topic": "Coding", "role": "Data Scientist"}
{"question": "What are dimensionality reduction and its benefits?", "answer": "Dimensionality reduction reduces the number of features while preserving information. Benefits: 1. Reduces storage and computation time. 2. Removes redundant features. 3. Improves model performance by reducing noise. Techniques include PCA and t-SNE.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "How will you calculate eigenvalues and eigenvectors of the following 3x3 matrix? [[-2, -4, 2], [-2, 1, 2], [4, 2, 5]]", "answer": "Solve the characteristic equation det(A - λI) = 0. For the matrix, the equation is λ^3 - 4λ^2 - 27λ + 90 = 0. Roots (eigenvalues) are λ = 3, -5, 6. For λ = 3, solve (A - 3I)x = 0 to get eigenvector [1, -3/2, -1/2]. Similarly, compute for other eigenvalues.", "difficulty": "hard", "topic": "Statistics", "role": "Data Scientist"}
{"question": "How should you maintain a deployed model?", "answer": "1. Monitor performance metrics continuously. 2. Evaluate model on new data. 3. Compare with alternative models. 4. Rebuild with updated data if performance degrades or data drifts.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you find RMSE and MSE in a linear regression model?", "answer": "MSE = (1/N) * Σ(predicted - actual)^2. RMSE = sqrt(MSE). They measure the average squared error and its square root, respectively, indicating model accuracy.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How can you select k for k-means?", "answer": "Use the elbow method: Plot within-cluster sum of squares (WSS) against k. Choose k where the WSS decrease slows, forming an ‘elbow’. Alternatively, use silhouette score to maximize cluster cohesion.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the significance of p-value?", "answer": "A p-value ≤ 0.05 indicates strong evidence against the null hypothesis (reject it). A p-value > 0.05 suggests weak evidence (fail to reject). A p-value ≈ 0.05 is marginal, requiring further investigation.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "How can outlier values be treated?", "answer": "1. Remove outliers if they’re errors. 2. Cap at percentiles (e.g., 1st/99th). 3. Use robust models (e.g., random forests). 4. Transform data (e.g., log transformation).", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "How can a time-series data be declared as stationary?", "answer": "Time-series data is stationary if its mean and variance are constant over time. Test using visual inspection (constant mean/variance) or statistical tests like ADF (Augmented Dickey-Fuller).", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "How can you calculate accuracy using a confusion matrix?", "answer": "Accuracy = (True Positives + True Negatives) / Total Observations. For a confusion matrix, it’s (TP + TN) / (TP + TN + FP + FN).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Write the equation and calculate the precision and recall rate.", "answer": "Precision = TP / (TP + FP). Recall = TP / (TP + FN). For a confusion matrix with TP=262, FP=15, FN=26, TN=347: Precision = 262/277 ≈ 0.94, Recall = 262/288 ≈ 0.90.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "‘People who bought this also bought…’ recommendations seen on Amazon are a result of which algorithm?", "answer": "Collaborative filtering, which predicts user preferences based on other users’ purchase or rating patterns.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a Generative Adversarial Network?", "answer": "A GAN consists of a generator (creates fake data) and a discriminator (distinguishes real from fake). They’re trained adversarially to generate realistic data, used in image synthesis and data augmentation.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "You are given a dataset on cancer detection. You have built a classification model and achieved an accuracy of 96 percent. Why shouldn’t you be happy with your model performance? What can you do about it?", "answer": "Cancer detection datasets are often imbalanced, so high accuracy may reflect bias toward the majority class. Focus on sensitivity, specificity, and F1-score. Improve by: 1. Using class weights. 2. Oversampling minority class (e.g., SMOTE). 3. Evaluating with ROC-AUC.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables?", "answer": "K-Nearest Neighbors (KNN) can impute missing values by finding the nearest neighbors based on other features, suitable for both categorical and continuous data.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the entropy of the target variable [0,0,0,1,1,1,1,1]?", "answer": "Entropy = -(p1*log(p1) + p0*log(p0)), where p1 = 5/8, p0 = 3/8. Entropy = -(5/8*log(5/8) + 3/8*log(3/8)). Correct answer: 1.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "We want to predict the probability of death from heart disease based on three risk factors: age, gender, and blood cholesterol level. What is the most appropriate algorithm?", "answer": "Logistic regression, as it predicts binary outcomes (death/no death) and handles probabilistic predictions based on continuous and categorical features.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "You have identified four specific individual types in a population study. Which algorithm is most appropriate to find similar users?", "answer": "K-means clustering, as it groups data into k clusters based on similarity, suitable for identifying users similar to predefined types.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "You have run the association rules algorithm and found {banana, apple} => {grape} and {apple, orange} => {grape} to be relevant. What else must be true?", "answer": "{grape, apple} must be a frequent itemset, as it’s part of both rules, ensuring sufficient support in the dataset.", "difficulty": "hard", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "Your organization has a website where visitors randomly receive one of two coupons or none. Which analysis method should you use to determine if coupons impact purchase decisions?", "answer": "One-way ANOVA, as it compares means across multiple groups (two coupon types and no coupon) to assess significant differences in purchase behavior.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What are feature vectors?", "answer": "Feature vectors are n-dimensional vectors of numerical features representing an object’s characteristics, used in machine learning to enable mathematical analysis of data.", "difficulty": "easy", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is root cause analysis?", "answer": "Root cause analysis identifies the underlying causes of problems by analyzing fault sequences, preventing recurrence. It’s used in industries to improve processes and outcomes.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What are the important skills to have in Python with regard to data analysis?", "answer": "Skills: 1. Mastery of data types (lists, dictionaries). 2. NumPy for array operations. 3. Pandas for dataframes. 4. Scikit-learn for modeling. 5. Writing efficient list comprehensions and functions. 6. Performance profiling and optimization.", "difficulty": "medium", "topic": "Coding", "role": "Data Scientist"}
{"question": "What is the difference between L1 and L2 regularization?", "answer": "L1 regularization (Lasso) adds the absolute values of coefficients to the loss, promoting sparsity for feature selection. L2 regularization (Ridge) adds squared coefficients, shrinking weights to handle multicollinearity. L1 is robust to outliers; L2 penalizes large weights more.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Write a SQL query to find the second-highest salary from an Employee table.", "answer": "SELECT MAX(salary) FROM Employee WHERE salary < (SELECT MAX(salary) FROM Employee); Alternatively: SELECT salary FROM Employee ORDER BY salary DESC LIMIT 1 OFFSET 1;", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is XGBoost and how does it differ from Random Forest?", "answer": "XGBoost is a gradient boosting framework that builds sequential trees, each correcting prior errors, optimized via gradient descent. Random Forest builds independent trees in parallel. XGBoost often outperforms on structured data but requires careful tuning to avoid overfitting.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you handle categorical variables in a machine learning model?", "answer": "1. One-hot encoding: Create binary columns for each category. 2. Label encoding: Assign integers to categories (for ordinal data). 3. Target encoding: Replace categories with mean target values. Choose based on model type and cardinality.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between Spark RDDs and DataFrames?", "answer": "RDDs are low-level, unstructured collections for parallel processing, requiring manual optimization. DataFrames are structured, tabular APIs with SQL-like operations and automatic optimization via Catalyst. DataFrames are easier and more efficient.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "How do you handle imbalanced datasets?", "answer": "1. Oversample minority class (e.g., SMOTE). 2. Undersample majority class. 3. Use class weights in loss functions. 4. Evaluate with metrics like F1-score, ROC-AUC instead of accuracy.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between bagging and boosting?", "answer": "Bagging (e.g., Random Forest) builds independent models in parallel and aggregates predictions, reducing variance. Boosting (e.g., XGBoost) builds sequential models, each correcting prior errors, reducing bias but risking overfitting.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How would you explain a machine learning model to a non-technical stakeholder?", "answer": "Explain the model as a tool that learns patterns from data to make predictions, like a recipe that improves with practice. Highlight its purpose (e.g., predicting sales) and benefits (e.g., better decisions), avoiding technical jargon.", "difficulty": "easy", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is principal component analysis (PCA)?", "answer": "PCA is a dimensionality reduction technique that transforms data into a new coordinate system of orthogonal components, ordered by variance. It reduces features while preserving most information, used for visualization and noise reduction.", "difficulty": "hard", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between Hadoop and Spark?", "answer": "Hadoop is a distributed storage (HDFS) and processing (MapReduce) framework, disk-based and slower. Spark is an in-memory processing framework, faster and more flexible, supporting iterative algorithms and SQL-like operations.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "How do you evaluate a regression model?", "answer": "Use metrics: 1. RMSE (error magnitude). 2. MAE (average error). 3. R-squared (explained variance). 4. Adjusted R-squared (penalizes complexity). Visualize residuals to check assumptions.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a confusion matrix?", "answer": "A confusion matrix is a 2x2 table (for binary classification) showing True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). It’s used to calculate metrics like accuracy, precision, and recall.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you perform feature scaling?", "answer": "Feature scaling standardizes feature ranges. Methods: 1. Standardization (zero mean, unit variance). 2. Min-max scaling (scale to [0,1]). Use for algorithms sensitive to scale (e.g., SVM, neural networks).", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is gradient boosting?", "answer": "Gradient boosting builds an ensemble of decision trees sequentially, where each tree corrects errors of the previous ones by minimizing a loss function using gradient descent. Examples: XGBoost, LightGBM.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you handle multicollinearity in regression models?", "answer": "1. Remove highly correlated features. 2. Use regularization (e.g., Ridge). 3. Apply PCA to reduce dimensionality. 4. Use VIF (Variance Inflation Factor) to detect and address multicollinearity.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the bias-variance tradeoff?", "answer": "The bias-variance tradeoff balances model complexity. High bias (simple models) leads to underfitting; high variance (complex models) leads to overfitting. The goal is a model with low bias and low variance for optimal performance.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How would you handle a situation where your model performs poorly in production?", "answer": "1. Investigate data drift or distribution changes. 2. Re-evaluate model metrics on production data. 3. Retrain with recent data. 4. Simplify model or try alternatives. 5. Communicate findings to stakeholders.", "difficulty": "hard", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is the difference between precision and recall?", "answer": "Precision is the ratio of true positives to predicted positives (TP / (TP + FP)), measuring prediction accuracy. Recall is the ratio of true positives to actual positives (TP / (TP + FN)), measuring the ability to find all positives.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is an F1-score?", "answer": "The F1-score is the harmonic mean of precision and recall, calculated as 2 * (precision * recall) / (precision + recall). It balances precision and recall, useful for imbalanced datasets.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a t-test and when is it used?", "answer": "A t-test compares the means of two groups to determine if they’re significantly different. Types: Independent t-test (two groups), paired t-test (same group, different conditions). Used when data is normally distributed.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the difference between a left join and an inner join in SQL?", "answer": "An inner join returns only rows where there’s a match in both tables. A left join returns all rows from the left table and matching rows from the right table, with NULLs for non-matches.", "difficulty": "easy", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the purpose of normalization in databases?", "answer": "Normalization organizes database tables to reduce redundancy and improve data integrity by dividing data into related tables and applying rules (normal forms). It ensures efficient storage and consistency.", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the difference between a parametric and non-parametric model?", "answer": "Parametric models assume a fixed form for the data distribution (e.g., linear regression). Non-parametric models make fewer assumptions, allowing flexibility (e.g., k-NN, decision trees), but require more data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the purpose of a validation curve?", "answer": "A validation curve plots a model’s performance (e.g., accuracy) against a range of hyperparameter values, helping identify optimal settings and diagnose underfitting or overfitting.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a learning curve in machine learning?", "answer": "A learning curve plots model performance (e.g., accuracy) against training set size, showing how performance improves with more data and diagnosing bias or variance issues.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between AdaBoost and Gradient Boosting?", "answer": "AdaBoost adjusts sample weights to focus on misclassified points, using weak learners with equal weights in predictions. Gradient Boosting builds trees sequentially, minimizing a differentiable loss function via gradient descent, offering more flexibility in loss functions.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Write a SQL query to find employees who earn more than their managers.", "answer": "SELECT e1.name FROM Employee e1 JOIN Employee e2 ON e1.manager_id = e2.id WHERE e1.salary > e2.salary;", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the purpose of a confusion matrix?", "answer": "A confusion matrix summarizes classification performance, showing True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). It’s used to calculate metrics like accuracy, precision, recall, and F1-score.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Explain the bias-variance decomposition.", "answer": "Bias-variance decomposition breaks down a model’s error into: Bias (error due to overly simple models), Variance (error due to sensitivity to training data), and irreducible error (noise). It helps balance underfitting and overfitting.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is k-fold cross-validation and how does it work?", "answer": "k-fold cross-validation splits the dataset into k subsets. The model trains on k-1 folds and tests on the remaining fold, repeating k times. Performance is averaged across all folds to estimate generalization.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between precision, recall, and F1-score?", "answer": "Precision = TP / (TP + FP), measuring prediction accuracy. Recall = TP / (TP + FN), measuring the ability to find positives. F1-score = 2 * (precision * recall) / (precision + recall), balancing both for imbalanced data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How would you handle missing values in a time-series dataset?", "answer": "Options: 1. Forward/backward fill to propagate values. 2. Interpolate using linear or spline methods. 3. Impute with rolling mean/median. 4. Use time-series models (e.g., ARIMA) to predict missing values.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What is the difference between Spark’s DataFrame and Dataset APIs?", "answer": "DataFrames are untyped, tabular data structures optimized for SQL-like operations. Datasets are typed, offering compile-time type safety and functional programming, used mainly in Scala. Both leverage Spark’s Catalyst optimizer.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "Write a Python function to calculate the Fibonacci sequence up to n.", "answer": "def fibonacci(n): a, b = 0, 1; result = []; for _ in range(n): result.append(a); a, b = b, a + b; return result", "difficulty": "easy", "topic": "Coding", "role": "Data Scientist"}
{"question": "What is the purpose of feature scaling?", "answer": "Feature scaling standardizes feature ranges to ensure equal contribution to distance-based algorithms (e.g., SVM, k-NN) and speeds up gradient-based optimization in neural networks.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between a left join and a right join in SQL?", "answer": "A left join returns all rows from the left table and matching rows from the right, with NULLs for non-matches. A right join returns all rows from the right table and matching rows from the left.", "difficulty": "easy", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is principal component analysis (PCA)?", "answer": "PCA transforms high-dimensional data into a lower-dimensional space by finding orthogonal components that maximize variance. It’s used for dimensionality reduction and visualization.", "difficulty": "hard", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "How do you detect multicollinearity in a regression model?", "answer": "Use Variance Inflation Factor (VIF): VIF > 5 or 10 indicates high multicollinearity. Alternatively, check correlation matrices for high pairwise correlations (>0.8).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between bagging and boosting?", "answer": "Bagging builds independent models in parallel (e.g., Random Forest) to reduce variance. Boosting builds sequential models (e.g., XGBoost) to reduce bias, each correcting previous errors.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the elbow method in k-means clustering?", "answer": "The elbow method plots within-cluster sum of squares (WSS) against the number of clusters (k). The ‘elbow’ point, where WSS decreases slowly, indicates the optimal k.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a t-test, and when is it used?", "answer": "A t-test compares the means of two groups to check for significant differences. Types: independent t-test (two groups), paired t-test (same group, different conditions). Used for normally distributed data.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "How would you handle a situation where your model performs poorly in production?", "answer": "Investigate data drift, re-evaluate metrics on production data, retrain with recent data, simplify the model, or try alternatives. Communicate findings to stakeholders and iterate.", "difficulty": "hard", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is the difference between L1 and L2 regularization?", "answer": "L1 (Lasso) adds absolute coefficient values to the loss, promoting sparsity. L2 (Ridge) adds squared coefficients, shrinking weights. L1 is robust to outliers; L2 handles multicollinearity.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Write a SQL query to find the top 5 customers by total order value.", "answer": "SELECT c.customer_id, SUM(o.order_amount) as total_value FROM customers c JOIN orders o ON c.customer_id = o.customer_id GROUP BY c.customer_id ORDER BY total_value DESC LIMIT 5;", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the purpose of a learning curve?", "answer": "A learning curve plots model performance against training set size, showing how performance improves with data and diagnosing high bias (underfitting) or high variance (overfitting).", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is gradient boosting, and how does it work?", "answer": "Gradient boosting builds an ensemble of decision trees sequentially, each minimizing a loss function via gradient descent. It corrects errors of prior trees, improving accuracy.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you handle categorical variables with high cardinality?", "answer": "Use target encoding (replace categories with mean target values), frequency encoding, or use tree-based models (e.g., XGBoost) that handle high cardinality naturally.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between Hadoop MapReduce and Spark?", "answer": "MapReduce is disk-based, slower, and suited for batch processing. Spark is in-memory, faster, and supports iterative and real-time processing with SQL-like APIs.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "What is the purpose of normalization in databases?", "answer": "Normalization organizes tables to reduce redundancy and improve integrity by applying normal forms, ensuring efficient storage and consistency.", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is a ROC curve, and how is it used?", "answer": "A ROC curve plots true positive rate vs. false positive rate at various thresholds. The AUC (area under the curve) measures model discrimination, with higher AUC indicating better performance.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "Write Python code to reverse a linked list.", "answer": "class ListNode: def __init__(self, val=0, next=None): self.val = val; self.next = next; def reverseList(head): prev = None; curr = head; while curr: next_temp = curr.next; curr.next = prev; prev = curr; curr = next_temp; return prev", "difficulty": "medium", "topic": "Coding", "role": "Data Scientist"}
{"question": "What is the difference between parametric and non-parametric models?", "answer": "Parametric models assume a fixed form (e.g., linear regression). Non-parametric models (e.g., k-NN, decision trees) are flexible, making fewer assumptions but requiring more data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a validation curve?", "answer": "A validation curve plots model performance against a hyperparameter range (e.g., regularization strength), helping identify optimal settings.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How do you evaluate a regression model?", "answer": "Use metrics: measures error magnitude. RMSE 2. MAE: average absolute error. MAE 3. R-squared: explained variance. R-squared 4. Adjusted R-squared: penalizes complexity. Visualize residuals to check assumptions.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a confusion matrix?", "answer": "A 2x2 table shows True Positives, True Negatives, False Positives, and False Negatives, used to calculate accuracy, precision, and recall.", "difficulty": "easy", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between supervised and unsupervised learning?", "answer": "Supervised learning uses labeled data to predict outcomes (e.g., classification). Unsupervised learning uses unlabeled data to find patterns (e.g., clustering).", "difficulty": "easy", "topic": "Supervised Learning", "role": "Data Scientist"}
{"question": "What is a random forest, and why is it effective?", "answer": "Random Forest is an ensemble of decision trees, each trained on a random subset of data and features. It reduces overfitting and improves accuracy through aggregation.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between oversampling and undersampling?", "answer": "Oversampling increases minority class samples (e.g., SMOTE) to balance data. Undersampling reduces majority class samples. Oversampling risks overfitting; undersampling risks data loss.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between a decision tree and a random forest?", "answer": "A Decision Tree is a single tree making decisions based on feature splits. Random Forest is an ensemble of trees, reducing overfitting by averaging predictions.", "difficulty": "medium", "topic": "Random Forest", "role": "Machine Learning"}
{"question": "How is the difference between a generative and discriminative model?", "answer": "Generative models (e.g., Naive Bayes) model joint probability P(X,Y), generating data. Discriminative models (e.g., logistic regression) model conditional probability P(Y|X), focusing on classification.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between batch and online learning?", "answer": "Batch learning trains on the entire dataset at once, suitable for static data. Online learning updates the model incrementally with new data, ideal for streaming data.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "How would you explain a machine learning model to a non-technical stakeholder?", "answer": "Describe it as a tool that learns patterns from data to predict outcomes, like a recipe improving with practice. Highlight its purpose and benefits without jargon.", "difficulty": "easy", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is the difference between a chi-square test and an ANOVA?", "answer": "Chi-square tests assess independence of categorical variables. ANOVA compares means across multiple groups for continuous variables. Use chi-square for counts, ANOVA for means.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "Write a SQL query to calculate the cumulative sum of sales by month.", "answer": "SELECT month, sales, SUM(sales) OVER (PARTITION BY year ORDER BY month) as cumulative_sales FROM sales_table;", "difficulty": "hard", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the difference between a data lake and a data warehouse?", "answer": "A data lake stores raw, semi-structured, and unstructured data, suited for flexible analysis. A data warehouse stores structured, processed data optimized for querying and reporting.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "What is the difference between a one-sample and two-sample t-test?", "answer": "A one-sample t-test compares a sample mean to a known population mean. A two-sample t-test compares means of two independent samples to check for differences.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is the purpose of SMOTE in handling imbalanced datasets?", "answer": "SMOTE (Synthetic Minority Oversampling Technique) generates synthetic samples of the minority class by interpolating between minority class points, balancing the dataset and improving model performance on minority classes.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is Python code to find the intersection of two lists?", "answer": "def intersection(lst1, lst2): return list(set(lst1) & set(lst2))", "difficulty": "easy", "topic": "Coding", "role": "Data Scientist"}
{"question": "What is the difference between feature selection and feature extraction?", "answer": "Feature selection chooses a subset of original features (e.g., Lasso). Feature extraction creates new features from originals (e.g., PCA). Selection preserves interpretability; extraction reduces dimensionality.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between a Markov chain and a hidden Markov model?", "answer": "A Markov chain models state transitions with observable states. A hidden Markov model includes hidden states and observed emissions with, used for speech recognition and bioinformatics.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is a SQL query to find duplicate records in a table?", "answer": "SELECT column_name, COUNT(*) as count FROM table_name GROUP BY column_name HAVING count > 1;", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the difference between batch gradient descent and mini-batch gradient descent?", "answer": "Batch gradient descent computes gradients using the entire dataset, stable but accurate. Mini-batch gradient descent uses small batches, balancing speed and stability.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between a convolutional neural network (CNN) and a fully connected neural network?", "answer": "CNNs use convolutional layers to extract spatial features, ideal for images. Fully connected networks connect all neurons, suitable for tabular data but less efficient for spatial data.", "difficulty": "medium", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between of a data scientist and a data engineer?", "answer": "Data scientists analyze data, build models, and derive insights. Data engineers design and maintain data pipelines, ensuring data quality and accessibility for analysis.", "difficulty": "easy", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is the difference between a one-hot encoding and label encoding?", "answer": "One-hot encoding creates binary columns for each category, avoiding ordinal bias. Label encoding assigns integers to categories, which may imply order, unsuitable for ordinal data.", "difficulty": "medium", "topic": "Feature Engineering", "role": "Data Scientist"}
{"question": "What is the difference between of a generative adversarial network (GAN) and how does it work?", "answer": "A GAN consists of a generator (creates fake data) and a discriminator (distinguishes real from fake), trained adversarially to generate realistic data, used in image synthesis.", "difficulty": "hard", "topic": "Deep Learning", "role": "Data Scientist"}
{"question": "What is the difference between a chi-square test and a t-test?", "answer": "Chi-square test assesses independence of categorical variables or goodness-of-fit. T-test compares means of continuous variables, either between groups or against a known value.", "difficulty": "medium", "topic": "Statistics", "role": "Data Scientist"}
{"question": "What is Python code to calculate the median of a list?", "answer": "def median(lst): lst.sort(); n = len(lst); if n % 2 == 0: return (lst[n//2-1] + lst[n//2])/2; else: return lst[n//2]", "difficulty": "easy", "topic": "Coding", "role": "Data Scientist"}
{"question": "What is the difference between a validation set, training set, and test set?", "answer": "Training set trains the model; validation set tunes hyperparameters; test set evaluates final performance. All are disjoint to ensure unbiased evaluation.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between a data warehouse and a database?", "answer": "A A database stores current, transactional data for operations. A data warehouse stores historical, aggregated, aggregated data for analytics and reporting, optimized for queries.", "difficulty": "medium", "topic": "Big Data", "role": "Data Scientist"}
{"question": "What is would you design a recommendation system for an e-commerce platform?", "answer": "Use collaborative filtering (user-item interactions) and content-based filtering (item features). Combine in a hybrid approach, use matrix factorization for scalability, and evaluate with RMSE or precision@k.", "difficulty": "hard", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is the difference between a false positive rate and a true positive rate?", "answer": "True positive rate Rate (recall) = TP / (TP + FN), measures positives correctly identified. False positive rate Rate = FP / (FP + TN), measures negatives incorrectly identified. as positives.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is would you handle a SQL query to find the running total of sales by region?", "answer": "SELECT region, sales_date, date, SUM(sales_amount) OVER (PARTITION BY region ORDER BY sales_date) as running_total FROM sales_table; sales;", "difficulty": "hard", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is the difference between normalization and denormalization in databases?", "answer": "Normalization reduces redundancy by organizing data into tables (normal forms). Denormalization reintroduces redundancy for query performance, common in data warehouses.", "difficulty": "medium", "topic": "SQL", "role": "Data Scientist"}
{"question": "What is a random walk model in time-series analysis?", "answer": "A random walk model assumes the next value equals the current value plus a random step, often used for non-stationary time series like stock prices.", "difficulty": "medium", "topic": "Data Analysis", "role": "Data Scientist"}
{"question": "What is the difference between a precision-recall curve and a ROC curve?", "answer": "Precision-recall curve plots precision vs. recall, useful for imbalanced datasets. ROC curve plots true positive rate vs. false positive rate, better for balanced datasets.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}
{"question": "What is would you handle a machine learning model that is biased against a protected group?", "answer": "Analyze feature importance to identify biases. Use fairness metrics (e.g., demographic, parity). Mitigate with reweighting, adversarial training, or post-processing. Communicate transparently.", "difficulty": "hard", "topic": "Behavioral", "role": "Data Scientist"}
{"question": "What is the difference between a decision boundary and a hyperplane?", "answer": "A decision boundary separates classes in a classifier, of any shape. A hyperplane is a linear boundary in high-dimensional space, used by SVMs or linear models.", "difficulty": "medium", "topic": "Machine Learning", "role": "Data Scientist"}