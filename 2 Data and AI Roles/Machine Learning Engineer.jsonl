{"question":"What is supervised machine learning?","answer":"Supervised learning is a type of machine learning where algorithms are trained on labeled data, with input data paired with corresponding output labels. The model learns to map inputs to outputs, enabling predictions on new data. Examples include Linear Regression, Logistic Regression, and k-Nearest Neighbors (kNN).","difficulty":"Easy","topic":"Supervised Machine Learning","role":"Machine Learning Engineer"}
{"question":"What is regression? Which models can you use to solve a regression problem?","answer":"Regression is a supervised machine learning task that predicts a continuous output variable based on input features. It models the relationship between a dependent (target) and independent variables (predictors). Common models include: Linear Regression (linear relationship), Polynomial Regression (non-linear, curved fit), Ridge Regression (handles multicollinearity with L2 regularization), and Lasso Regression (feature selection with L1 regularization).","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is linear regression? When do we use it?","answer":"Linear regression is a model that assumes a linear relationship between input variables (X) and a single output variable (y), expressed as y = B0 + B1*x1 + ... + Bn*xn, where B are coefficients. It’s used to predict numeric outcomes when the relationship between features and target is approximately linear, e.g., predicting house prices or sales.","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What are the main assumptions of linear regression?","answer":"Linear regression assumes: 1) Linear relationship between features and target. 2) Additivity: effects of features are independent. 3) No collinearity: features are not highly correlated. 4) Errors are independently and identically normally distributed with constant variance (homoscedasticity) and no correlation. Violations can lead to unreliable predictions or misinterpretation.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What’s the normal distribution? Why do we care about it?","answer":"The normal distribution is a continuous probability distribution with a bell-shaped curve, defined by mean (μ) and standard deviation (σ). It’s important due to the Central Limit Theorem, which states that the mean of large, independent samples approximates a normal distribution, enabling statistical analysis of unknown distributions.","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"How do we check if a variable follows the normal distribution?","answer":"To check normality: 1) Plot a histogram and fit a bell curve. 2) Calculate skewness (should be ~0) and kurtosis (~3). 3) Use statistical tests like Kolmogorov-Smirnov or Shapiro-Wilk. 4) Create a Q-Q plot; a straight line indicates normality. These methods assess if data aligns with a normal distribution.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices?","answer":"Prices are typically not normally distributed, often exhibiting right-skewness due to outliers (e.g., luxury items). Pre-processing is needed, such as removing outliers or applying transformations (e.g., log transformation) to make the distribution more normal, improving model performance for algorithms like linear regression.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What methods for solving linear regression do you know?","answer":"Methods to solve linear regression include: 1) Matrix Algebra: Using the normal equation, β = (X^T X)^(-1) X^T y, often solved with Singular Value Decomposition (SVD) or QR Decomposition for stability. 2) Gradient Descent: Iteratively updates coefficients to minimize the loss function, suitable for large datasets.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is gradient descent? How does it work?","answer":"Gradient descent is an iterative optimization algorithm that minimizes a loss function. It computes the gradient of the loss with respect to model parameters and updates parameters in the opposite direction, scaled by a learning rate, until reaching a local or global minimum. It’s widely used in machine learning.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is the normal equation?","answer":"The normal equation is an analytical solution to linear regression, obtained by setting the partial derivatives of the sum of squared errors to zero. It’s expressed as β = (X^T X)^(-1) X^T y, providing the coefficients that minimize the loss, but it’s computationally expensive for large datasets.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is SGD — stochastic gradient descent? What’s the difference with the usual gradient descent?","answer":"Stochastic Gradient Descent (SGD) updates model parameters using the gradient of the loss for one or a small subset of training samples, unlike standard Gradient Descent (GD), which uses all samples. SGD is faster for large datasets but noisier; GD is more precise but slower.","difficulty":"Medium","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"Which metrics for evaluating regression models do you know?","answer":"Common regression metrics include: 1) Mean Squared Error (MSE), 2) Root Mean Squared Error (RMSE), 3) Mean Absolute Error (MAE), 4) R² (Coefficient of Determination), and 5) Adjusted R². These assess prediction accuracy and model fit.","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What are MSE and RMSE?","answer":"Mean Squared Error (MSE) is the average of squared differences between predicted and actual values, emphasizing larger errors. Root Mean Squared Error (RMSE) is the square root of MSE, providing error in the same units as the target, making it more interpretable.","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is the bias-variance trade-off?","answer":"The bias-variance trade-off balances model complexity. Bias is error from overly simple models (underfitting); variance is sensitivity to training data changes (overfitting). Expected test error = Variance + Bias² + Irreducible Error. Optimal models minimize both bias and variance.","difficulty":"Easy","topic":"Linear Regression","role":"Machine Learning Engineer"}
{"question":"What is overfitting?","answer":"Overfitting occurs when a model performs well on training data but poorly on unseen test data, as it learns noise or specific patterns in the training set rather than generalizing. It’s caused by excessive complexity or insufficient regularization.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"How to validate your models?","answer":"Validate models by: 1) Splitting data into train, validation, and test sets; train on train data, tune hyperparameters on validation, and evaluate on test. 2) Cross-validation: Divide data into K folds, train on K-1, validate on the remaining fold, and average results. Combine both for robust evaluation.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"Why do we need to split our data into three parts: train, validation, and test?","answer":"Splitting data into train, validation, and test sets ensures: 1) Training set fits the model. 2) Validation set tunes hyperparameters for generalization. 3) Test set provides an unbiased final evaluation. This prevents overfitting and ensures realistic performance assessment.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"Can you explain how cross-validation works?","answer":"Cross-validation splits the dataset into K folds. For each fold, train the model on K-1 folds and validate on the remaining fold. Repeat K times, averaging performance metrics. This reduces bias from a single validation set and maximizes data use.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"What is K-fold cross-validation?","answer":"K-fold cross-validation divides the dataset into K equal parts. Each part is used as a validation set once, with the remaining K-1 parts as the training set. Performance is averaged over K iterations, providing a robust estimate of model performance.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"How do we choose K in K-fold cross-validation? What’s your favorite K?","answer":"Choose K based on dataset size and computational cost. Larger K (e.g., 10) reduces bias but increases computation; smaller K (e.g., 4) is faster but less reliable. A K of 5 is often used for large datasets, balancing bias and efficiency.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"What is classification? Which models would you use to solve a classification problem?","answer":"Classification predicts discrete class labels. Models include: Logistic Regression (binary), Decision Trees, Random Forests, Support Vector Machines (SVM), Neural Networks, and k-Nearest Neighbors (kNN). Choice depends on data size, complexity, and interpretability needs.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is logistic regression? When do we need to use it?","answer":"Logistic regression is a binary classification algorithm that predicts the probability of a class using a sigmoid function. It’s used when the target variable is binary (e.g., spam/not spam, churn/not churn) and a probabilistic output is desired.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"Is logistic regression a linear model? Why?","answer":"Yes, logistic regression is a generalized linear model. The output (log-odds) is a linear combination of inputs and parameters, transformed by the sigmoid function, ensuring the model captures linear relationships in the log-odds space.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is sigmoid? What does it do?","answer":"The sigmoid function, Sigmod(x) = 1/(1+e^(-x)), maps any real number to (0,1). In logistic regression, it converts linear combinations of features into probabilities, enabling binary classification by thresholding (e.g., 0.5).","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"How do we evaluate classification models?","answer":"Evaluate classification models using: 1) Accuracy, 2) Precision, 3) Recall, 4) F1 Score, 5) Cross-entropy loss, 6) Jaccard similarity. Metrics depend on the problem, e.g., precision/recall for imbalanced data.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is accuracy?","answer":"Accuracy is the ratio of correct predictions to total predictions in a classification model. It’s a simple metric but can be misleading for imbalanced datasets where one class dominates.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"Is accuracy always a good metric?","answer":"No, accuracy is not always good, especially for imbalanced datasets. For example, in a dataset with 95% class A, predicting A always yields 95% accuracy but misses class B. Use precision, recall, or F1-score for imbalanced data.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is the confusion table? What are the cells in this table?","answer":"A confusion matrix shows classification performance with: True Positives (TP: predicted and actual positive), True Negatives (TN: predicted and actual negative), False Positives (FP: predicted positive, actual negative), False Negatives (FN: predicted negative, actual positive).","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What are precision, recall, and F1-score?","answer":"Precision = TP/(TP+FP), measuring correct positive predictions. Recall = TP/(TP+FN), measuring captured positive instances. F1-score = 2*Precision*Recall/(Precision+Recall), the harmonic mean, balancing precision and recall. All range from 0 to 1, with 1 being best.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"Precision-recall trade-off","answer":"The precision-recall trade-off occurs when increasing one metric decreases the other due to shifting decision thresholds in non-perfectly separable data. Higher precision reduces false positives but may miss true positives (lower recall), and vice versa. Adjust thresholds based on problem needs.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is the ROC curve? When to use it?","answer":"The ROC (Receiver Operating Characteristic) curve plots True Positive Rate (Recall) vs. False Positive Rate at various thresholds. Use it to evaluate binary classifiers, especially when comparing models or assessing performance across thresholds.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is AUC (AU ROC)? When to use it?","answer":"AUC (Area Under the ROC Curve) measures a classifier’s ability to distinguish classes, ranging from 0 to 1 (higher is better). Use it to compare models or evaluate performance, especially for balanced datasets.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"How to interpret the AU ROC score?","answer":"AUC = 1 indicates perfect class separation; AUC = 0.5 means no separation (random guessing); AUC < 0.5 suggests worse-than-random performance. Higher AUC reflects better model ability to distinguish between classes.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is the PR (precision-recall) curve?","answer":"The Precision-Recall (PR) curve plots precision vs. recall at different thresholds. It’s useful for imbalanced datasets, highlighting trade-offs in positive class prediction performance.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is the area under the PR curve? Is it a useful metric?","answer":"Area Under the PR Curve (AU PR) summarizes precision-recall performance. High AU PR indicates both high precision and recall, useful for imbalanced datasets where positive class performance is critical.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"In which cases AU PR is better than AU ROC?","answer":"AU PR is better than AU ROC for imbalanced datasets or when the positive class is more important, as it focuses on precision and recall (positive predictive value and true positive rate) rather than true negative rate, which AU ROC includes.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What do we do with categorical variables?","answer":"Categorical variables must be encoded: 1) One-hot encoding (binary vectors), 2) Label encoding (integer labels), 3) Ordinal encoding (ordered integers), 4) Target encoding (mean target value). Choice depends on model and variable cardinality.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"Why do we need one-hot encoding?","answer":"One-hot encoding prevents ordinal assumptions in categorical variables, representing each category as a binary vector. This ensures equal distances between categories, avoiding issues in linear models where label encoding might imply order.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is 'curse of dimensionality'?","answer":"The curse of dimensionality refers to issues in high-dimensional data, where sparsity increases, requiring exponentially more data to model relationships. It causes overfitting, computational complexity, and poor generalization, mitigated by dimensionality reduction.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What happens to our linear regression model if we have three columns in our data: x, y, z — and z is a sum of x and y?","answer":"Linear regression fails because z = x + y creates perfect multicollinearity, making the feature matrix (X^T X) singular and non-invertible, preventing coefficient estimation.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise?","answer":"This causes multicollinearity, where x, y, and z are highly correlated, making coefficient estimation unstable. Use feature selection, PCA, or L2 regularization (Ridge) to mitigate.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What is regularization used for?","answer":"Regularization reduces overfitting by adding a penalty to the model’s loss function, improving generalization and making models robust to noise and outliers.","difficulty":"Easy","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"Which regularization techniques do you know?","answer":"Main regularization techniques: 1) L1 (Lasso) adds absolute coefficient sums, promoting sparsity. 2) L2 (Ridge) adds squared coefficient sums, reducing coefficient variance. Both use a λ penalty parameter.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What kind of regularization techniques are applicable to linear models?","answer":"Applicable to linear models: Ridge, Lasso, Elastic Net, AIC/BIC, Basis Pursuit Denoising, Dantzig Selector, SLOPE. These address overfitting, multicollinearity, or feature selection.","difficulty":"Easy","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"How does L2 regularization look like in a linear model?","answer":"L2 adds a penalty term to the cost function, λΣw_i², where λ is a hyperparameter. It shrinks coefficients toward zero, reducing variance in correlated feature scenarios.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"How do we select the right hyperparameter for regularization?","answer":"Select regularization parameters (e.g., λ) using grid search or random search, testing values and choosing the one with the lowest cross-validation or validation error.","difficulty":"Easy","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What’s the effect of L2 regularization on the weights of a linear model?","answer":"L2 regularization shrinks weights toward zero, penalizing larger weights more, reducing model variance and preventing overfitting, but not setting weights to zero.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"How does L1 regularization look like in a linear model?","answer":"L1 regularization adds a penalty term λΣ|w_i| to the cost function, promoting sparsity by driving some weights to exactly zero, enabling feature selection.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What’s the difference between L2 and L1 regularization?","answer":"L1 uses absolute weights, promoting sparsity (feature selection); L2 uses squared weights, shrinking weights but not to zero. L2 has an analytical solution and handles multicollinearity better; L1 is computationally intensive but sparse.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"Can we have both L1 and L2 regularization components in a linear model?","answer":"Yes, Elastic Net combines L1 and L2 regularization, balancing sparsity and coefficient shrinkage for improved performance in high-dimensional data.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What’s the interpretation of the bias term in linear models?","answer":"The bias term (intercept) represents the predicted output when all features are zero, adjusting the model’s baseline prediction to fit data better.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"How do we interpret weights in linear models?","answer":"Weights indicate the change in the output per unit change in a feature. For normalized features, they reflect feature importance; otherwise, scale affects interpretation.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"If a weight for one variable is higher than for another — can we say that this variable is more important?","answer":"Yes, for normalized features, higher weights indicate greater importance. Without normalization, scale differences (e.g., GDP vs. small units) can mislead interpretation.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"When do we need to perform feature normalization for linear models? When it’s okay not to do it?","answer":"Normalization is needed for L1 and L2 regularization to ensure equal penalty scaling. It’s optional without regularization, as linear models handle unscaled features, though normalization can improve numerical stability.","difficulty":"Medium","topic":"Regularization","role":"Machine Learning Engineer"}
{"question":"What is feature selection? Why do we need it?","answer":"Feature selection identifies relevant features for model training, removing irrelevant ones to improve performance, reduce overfitting, and enhance generalization.","difficulty":"Easy","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"Is feature selection important for linear models?","answer":"Yes, feature selection improves linear model performance by selecting relevant features, reducing noise, overfitting, and addressing multicollinearity issues.","difficulty":"Medium","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"Which feature selection techniques do you know?","answer":"Techniques include: Principal Component Analysis (PCA), Neighborhood Component Analysis (NCA), ReliefF, Filter methods (e.g., variance threshold), and Wrapper methods (e.g., recursive feature elimination).","difficulty":"Medium","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"Can we use L1 regularization for feature selection?","answer":"Yes, L1 regularization (Lasso) promotes sparsity by setting some coefficients to zero, effectively selecting non-zero features.","difficulty":"Medium","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"Can we use L2 regularization for feature selection?","answer":"No, L2 regularization shrinks coefficients but doesn’t set them to zero, so it’s unsuitable for feature selection, though it stabilizes models.","difficulty":"Medium","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"What are the decision trees?","answer":"Decision trees are supervised learning algorithms for classification and regression, splitting data into homogeneous groups based on feature thresholds, forming a flowchart-like structure with nodes and leaves.","difficulty":"Easy","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"How do we train decision trees?","answer":"Train decision trees by: 1) Starting at the root. 2) Finding the split to minimize child node impurities (e.g., Gini). 3) Recursively splitting until a stopping criterion (e.g., max depth) is met.","difficulty":"Medium","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"What are the main parameters of the decision tree model?","answer":"Key parameters: max_depth (tree depth), min_samples_split (minimum samples to split), min_samples_leaf (minimum samples in leaf), and impurity criterion (e.g., Gini, entropy).","difficulty":"Easy","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"How do we handle categorical variables in decision trees?","answer":"Categorical variables can be handled natively in some algorithms or encoded (e.g., one-hot or label encoding) for others. Trees split based on category thresholds.","difficulty":"Medium","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"What are the benefits of a single decision tree compared to more complex models?","answer":"Benefits: Easy to implement, fast training/prediction, interpretable. Suitable for simpler tasks but may underperform complex models on intricate data.","difficulty":"Medium","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"How can we know which features are more important for the decision tree model?","answer":"Feature importance is measured by how much a feature reduces impurity (e.g., Gini, entropy) across splits. Higher reduction indicates greater importance.","difficulty":"Medium","topic":"Decision Trees","role":"Machine Learning Engineer"}
{"question":"What is random forest?","answer":"Random Forest is an ensemble method combining multiple decision trees, using bagging and random feature sampling, for regression or classification, reducing variance.","difficulty":"Easy","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"Why do we need randomization in random forest?","answer":"Randomization (data and feature sampling) reduces variance by averaging independent trees and decorrelates them, improving generalization and preventing overfitting.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What are the main parameters of the random forest model?","answer":"Key parameters: max_depth, min_samples_split, max_leaf_nodes, min_samples_leaf, n_estimators (trees), max_samples_split (data fraction), max_features (feature subset).","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"How do we select the best hyperparameters in hyperparameter for hyperparameter random forest?"}
{"question":"How do we know how many trees we need in random forest?","answer":"The number of trees (n_estimators) is tuned to reduce overfitting. Start with sqrt(n_features), increase until performance stabilizes, using cross-validation. Typically, 100–500 trees suffice, balancing accuracy and computation.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"Is it easy to train a random forest model? How can we do it?","answer":"Yes, random forests are easy to parallelize. In R, use the `multicombine=TRUE` parameter with `foreach` to distribute tree training across cores, e.g., training multiple trees with `randomForest(x, y, n_estimators)`.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What are the problems with many large trees?","answer":"Issues include: 1) Overfitting, capturing noise. 2) Slow prediction times. 3) High memory usage. 4) Reduced interpretability. 5) Complex tuning. Balance tree size and number based on data and performance needs.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What if instead of randomization we randomly select a few splits and pick the best from them? Will it work?","answer":"Yes, this randomized split selection, used in Extra-Trees, samples splits and chooses the best, reducing computation and variance. It works in ensembles but may miss optimal splits.","difficulty":"Hard","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What happens when we have correlated features in our data?","answer":"Correlated features in Random Forests increase the likelihood of selecting redundant information, reducing model robustness. They may cause inconsistent feature importance and overfitting. Use feature selection or PCA to mitigate.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What is gradient boosting?","answer":"Gradient Boosting builds an ensemble of weak learners (e.g., trees) sequentially, each correcting prior errors by minimizing a loss function via gradient descent, used for regression and classification.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"What’s the difference between random forests and gradient boosting?","answer":"Random Forests build independent trees in parallel, averaging predictions. Gradient Boosting builds trees sequentially, each correcting errors, combining results iteratively. Random Forests reduce variance; Boosting reduces bias and variance.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"Is it possible to train a gradient boosting model? How to do it?","answer":"Yes, parallelization is possible using frameworks like XGBoost with `tree_method='hist'` or GPU support. Distribute tree construction across cores or GPUs for faster training.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"Feature importance in gradient boosting trees — what are the options?","answer":"Options include: 1) SHAP values (e.g., CatBoost’s method) for interpretable importance. 2) Adding a noise column and excluding features with lower importance than noise. Higher values indicate greater feature contribution.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"Are there differences between continuous and discrete variables for feature importance in gradient boosting models? ","answer":"Yes, gradient boosting treats continuous and discrete variables differently. Continuous variables are split at numerical thresholds, potentially leading to more splits and higher importance due to finer granularity. Discrete variables (categorical) are split based on categories, often requiring encoding (e.g., one-hot). Some algorithms (e.g., CatBoost) handle categorical variables natively, reducing bias. Importance metrics (e.g., gain, SHAP) may favor continuous variables if they capture more variance, but proper preprocessing ensures fair comparison.","difficulty":"Hard","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"What are the parameters in Gradient Boosting model?","parameters":"learning rate (0.1), n_estimators (100), max_depth (3), min_samples_split (2), min_samples_leaf (1), sample (1.0). Tune these to control model complexity and speed.","gradient":"Boosting","role":"Machine Learning Engineer"}
{"question":"How do you approach tuning in XGBoost or LightGBM?","answer":"Tune manually (e.g., max_depth, min_samples_leaf) or use frameworks like Optuna/Hyperopt. Start with conservative values to avoid overfitting, then use grid/random search to optimize based on validation metrics.","hyperparameter":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"How do you select the number of trees in a gradient boosting model?","answer":"Use a default (e.g., 100–1000 trees) and perform grid search with cross-validation to find the optimal n_estimators, balancing performance and computation time.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"Which hyper-parameter tuning strategies (in general) do you know?","answer":"Strategies include: 1) Grid Search (exhaustive combinations), 2) Random Search (random sampling), Hyperparameter Optimization (Bayesian, using statistical priors). Grid is thorough but slow; Random is efficient; Bayesian is best for complex models.","difficulty":"Medium","topic":"Parameter Tuning","role":"Machine Learning Engineer"}
{"question":"What’s the difference between grid search parameter and random search?","answer":"Grid Search tests all hyperparameter combinations; Random Search samples randomly within bounds. Grid is exhaustive but slow; Random is faster, better for large spaces without prior knowledge.","difficulty":"Medium","topic":"Parameter Tuning","role":"Machine Learning Engineer"}
{"question":"What kind of problems neural networks can solve?","answer":"Neural networks excel at non-linear problems like image classification, speech recognition, and NLP, where traditional regression fails due to complex patterns.","difficulty":"Easy","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How does a fully-connected neural network work?","answer":"Fully-connected feed-forward networks pass inputs through layers where each neuron connects to all previous layer neurons, applying weighted sums and activation functions to learn complex mappings.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"Why do we need activation functions?","answer":"Activation functions introduce non-linearity, enabling neural networks to learn complex, non-linear relationships. Without them, stacked linear layers reduce to a single linear function.","difficulty":"Easy","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What are the problems with sigmoid as an activation function?","answer":"Sigmoid’s derivatives near extremes are near zero, causing vanishing gradients, slowing or stopping learning. ReLU avoids this by maintaining non-zero gradients for positive inputs.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is ReLU? How is it better than sigmoid or tanh?","answer":"ReLU (Rectified Linear Unit) outputs x for x>0, else 0. It’s faster to compute, avoids vanishing gradients, and allows unbounded activation, unlike sigmoid/tanh, which saturate.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How we can initialize the weights of a neural network?","answer":"Initialize weights randomly (e.g., Xavier/Glorot) to break symmetry, avoiding zero initialization. Biases can be zero. Large/small weights cause exploding/vanishing gradients, slowing learning.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What if we set all the weights of a neural network to 0?","answer":"Zero weights produce identical outputs and gradients, causing all neurons to learn the same thing, preventing convergence. Random initialization avoids this.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What regularization techniques for neural nets do you know?","answer":"Techniques: L1 (sparsity), L2 (weight decay), Data Augmentation (diversify data), Dropout (randomly deactivate neurons). Dropout is highly effective for generalization.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is dropout? Why is it useful? How does it work?","answer":"Dropout randomly deactivates neurons during training with probability p, forcing the network to learn robust features. It reduces overfitting by preventing reliance on specific neurons.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is backpropagation? How does it work? Why do we need it?","answer":"Backpropagation computes gradients of the loss function w.r.t. weights using the chain rule, updating weights via gradient descent. It’s essential for training neural networks by minimizing errors.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"Which optimization techniques for training neural nets do you know?","answer":"Techniques: Gradient Descent, Stochastic Gradient Descent (SGD), Mini-Batch SGD, Momentum, Nesterov Accelerated Gradient, Adagrad, AdaDelta, Adam (fastest, most efficient).","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"How do we use SGD (stochastic gradient descent) for training a neural net?","answer":"SGD updates weights using gradients from one or a few samples, reducing computation for large datasets. It’s noisy but efficient, converging with proper learning rate tuning.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"What’s the learning rate?","answer":"The learning rate controls the step size of weight updates during gradient descent. It determines how quickly a model adapts to the problem.","difficulty":"Easy","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"What happens when the learning rate is too large? Too small?","answer":"Large learning rates may overshoot minima, causing divergence. Small learning rates slow convergence, risking local minima traps. Optimal rates balance speed and accuracy.","difficulty":"Easy","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"How to set the learning rate?","answer":"Start with a small rate (e.g., 0.01), adjust via trial and error, or use adaptive methods (e.g., Adam). Cross-validation helps find the optimal rate.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is Adam? What’s the main difference between Adam and SGD?","answer":"Adam (Adaptive Moment Estimation) combines momentum and adaptive learning rates, converging faster than SGD. SGD uses a fixed rate and is slower but may find better optima.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"When would you use Adam and when SGD?","answer":"Use Adam for faster convergence in deep learning. Use SGD for potentially better optima in simpler models or when fine-tuning with small datasets.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"Do we want to have a constant learning rate or we better change it throughout training?","answer":"Varying the learning rate (e.g., decaying with StepLR in PyTorch) is better. Start high for fast progress, then reduce to avoid overshooting and refine convergence.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"How do we decide when to stop training a neural net?","answer":"Stop training when validation error stabilizes or increases (early stopping), indicating overfitting. Monitor metrics like loss or accuracy on a validation set.","difficulty":"Easy","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is model checkpointing?","answer":"Model checkpointing saves model weights during training, allowing resumption from a specific point. It’s useful for long training or recovering from interruptions.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"Can you tell us how you approach the model training process?","answer":"1) Preprocess data (clean, normalize, encode). 2) Split into train/validation/test sets. 3) Choose a model and initialize weights. 4) Train with an optimizer (e.g., Adam), tuning hyperparameters via cross-validation. 5) Monitor validation metrics, using early stopping or checkpointing. 6) Evaluate on test set and iterate.","difficulty":"Medium","topic":"Optimization in Neural Networks","role":"Machine Learning Engineer"}
{"question":"How we can use neural nets for computer vision?","answer":"Use Convolutional Neural Networks (CNNs) for computer vision. CNNs extract spatial features via convolutions, effective for tasks like image classification, object detection, and segmentation.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What’s a convolutional layer?","answer":"A convolutional layer applies learnable kernels to input data, computing weighted sums over local regions. It reduces parameters via weight sharing and extracts spatial features.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"Why do we actually need convolutions? Can’t we use fully-connected layers for that?","answer":"Convolutions reduce parameters by sharing weights and focusing on local patterns, unlike fully-connected layers, which scale poorly with image size and lose spatial information.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What’s pooling in CNN? Why do we need it?","answer":"Pooling downsamples feature maps, reducing spatial dimensions while retaining important features. It lowers computation, prevents overfitting, and enables learning hierarchical features.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"How does max pooling work? Are there other pooling techniques?","answer":"Max pooling selects the maximum value in a receptive field (e.g., 2x2). Other techniques: Average pooling (mean value), Min pooling (minimum), Global pooling (entire region).","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated?","answer":"CNNs aren’t inherently rotation-resistant. Rotated images may change predictions unless trained with rotated data augmentations, which improve robustness.","difficulty":"Hard","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What are augmentations? Why do we need them?","answer":"Augmentations transform data (e.g., rotations, flips) to increase dataset diversity, improving model generalization and robustness, especially with limited data.","difficulty":"Easy","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What kind of augmentations do you know?","answer":"Augmentations include geometric (rotation, flip, crop), color (brightness, contrast), noise injection, PCA, padding, and shifting. Choice depends on task requirements.","difficulty":"Easy","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"How to choose which augmentations to use?","answer":"Choose augmentations based on task and data characteristics, e.g., rotations for orientation-invariant tasks or color shifts for varying lighting conditions.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What kind of CNN architectures for classification do you know?","answer":"Architectures: Inception v3, Xception, DenseNet, AlexNet, VGG16, ResNet, SqueezeNet, EfficientNet, MobileNet. The last three are lightweight for edge devices.","difficulty":"Hard","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What is transfer learning? How does it work?","answer":"Transfer learning reuses a pre-trained model’s weights (e.g., on ImageNet) for a new task, fine-tuning or freezing layers to adapt to new data, saving training time.","difficulty":"Medium","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What is object detection? Do you know any architectures for that?","answer":"Object detection locates and classifies objects in images with bounding boxes. Architectures: YOLO, Faster R-CNN, CenterNet.","difficulty":"Hard","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"What is object segmentation? Do you know any architectures for that?","answer":"Object segmentation predicts pixel-level masks for objects. Architectures: Mask R-CNN, UNet. It doesn’t distinguish individual object instances.","difficulty":"Hard","topic":"Neural Networks for Computer Vision","role":"Machine Learning Engineer"}
{"question":"How can we use machine learning for text classification?","answer":"Extract numerical features from text (e.g., bag of words, N-grams, embeddings) and apply classification algorithms like logistic regression, SVM, or neural networks.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What is bag of words? How we can use it for text classification?","answer":"Bag of Words represents text as word frequency histograms, ignoring order. For classification, treat word counts as features for models like logistic regression.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What are the advantages and disadvantages of bag of words?","answer":"Advantages: Simple, interpretable. Disadvantages: Ignores word order/context, sparse representations, and requires careful vocabulary design to manage size.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What are N-grams? How can we use them?","answer":"N-grams are consecutive word sequences of length N. Use them to capture local context in text classification by treating N-grams as features.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"How large should be N for our bag of words when using N-grams?","answer":"N typically ranges from 1 to 3 (unigrams, bigrams, trigrams). Larger N captures more context but increases sparsity and computation. Choose based on task complexity and data size.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What is TF-IDF? How is it useful for text classification?","answer":"TF-IDF (Term Frequency-Inverse Document Frequency) scores words based on frequency in a document (TF) and rarity across documents (IDF). It highlights distinctive words, improving text classification by weighting informative features.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"Which model would you use for text classification with bag of words features?","answer":"Models: Logistic Regression, Naive Bayes, SVM, Random Forests, CNNs, LSTMs, BERT. Logistic Regression is fast for high-dimensional bag of words.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words?","answer":"Logistic Regression is preferred for bag of words due to faster training on high-dimensional data. Gradient Boosting is slower but may capture complex patterns.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What are word embeddings? Why are they useful? Do you know Word2Vec?","answer":"Word embeddings are vector representations capturing word semantics. They’re useful for preserving context and similarity. Word2Vec uses CBOW or Skip-Gram to generate embeddings from text corpora.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"Do you know any other ways to get word embeddings?","answer":"Other methods: GloVe (global word co-occurrence), BERT (contextual embeddings), FastText (subword information). Each suits different tasks based on context needs.","difficulty":"Hard","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it?","answer":"Combine embeddings by: 1) Averaging word vectors. 2) Weighted averaging (e.g., IDF weights). 3) Using models like LSTM or Transformers to capture sequence information.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings?","answer":"Gradient Boosting Trees are preferred for embeddings, as they capture non-linear relationships in dense features. Logistic Regression is simpler but limited to linear patterns.","difficulty":"Medium","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"How can you use neural nets for text classification?","answer":"Preprocess text, convert to vectors (e.g., embeddings), build a neural network (e.g., CNN, LSTM, Transformer), train on labeled data, and evaluate on a test set for tasks like sentiment analysis.","difficulty":"Hard","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"How can we use CNN for text classification?","answer":"Use CNNs by treating text as a 1D sequence of word embeddings, applying convolutional layers to extract local patterns, followed by pooling and dense layers for classification.","difficulty":"Hard","topic":"Text Classification","role":"Machine Learning Engineer"}
{"question":"What is unsupervised learning?","answer":"Unsupervised learning detects patterns in unlabeled data, without predefined outputs, e.g., clustering or dimensionality reduction.","difficulty":"Easy","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"What is clustering? When do we need it?","answer":"Clustering groups similar data points into clusters based on features. Use it for pattern discovery, customer segmentation, or anomaly detection in unlabeled data.","difficulty":"Easy","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"Do you know how K-means works?","answer":"K-means: 1) Initialize K centroids. 2) Assign points to nearest centroid. 3) Update centroids as cluster means. 4) Repeat until convergence.","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"How to select K for K-means?","answer":"Select K using: 1) Domain knowledge. 2) Elbow method (plot within-cluster sum of squares vs. K). 3) Silhouette method (maximize average silhouette score).","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"What are the other clustering algorithms do you know?","answer":"Algorithms: k-medoids (robust to noise), Agglomerative Hierarchical Clustering (bottom-up), DIANA (top-down), DBSCAN (density-based, handles noise).","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"Do you know how DBScan works?","answer":"DBSCAN clusters density-connected points using parameters ε (radius) and minPts. Points within ε are density-reachable, forming clusters; others are noise.","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"When would you choose K-means and when DBScan?","answer":"Choose K-means for large, spherical clusters and speed. Use DBSCAN for noisy data, irregular clusters, or when K is unknown.","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"What is the curse of dimensionality? Why do we care about it?","answer":"The curse of dimensionality is data sparsity in high-dimensional spaces, increasing overfitting and computation. It’s critical because it degrades model performance.","difficulty":"Medium","topic":"Dimensionality Reduction","role":"Machine Learning Engineer"}
{"question":"Do you know any dimensionality reduction techniques?","answer":"Techniques: Singular Value Decomposition (SVD), PCA, Linear Discriminant Analysis (LDA), t-SNE, Autoencoders, Fourier/Wavelet Transforms.","difficulty":"Medium","topic":"Dimensionality Reduction","role":"Machine Learning Engineer"}
{"question":"What’s singular value decomposition? How is it used for machine learning?","answer":"SVD decomposes a matrix into X = LΣR^T. In PCA, a special SVD, it reduces dimensionality by selecting principal components based on eigenvalues, improving efficiency.","difficulty":"Medium","topic":"Dimensionality Reduction","role":"Machine Learning Engineer"}
{"question":"What is the ranking problem? Which models can you use to solve them?","answer":"Ranking problems order items by relevance (e.g., search results). Models: RankSVM, Gradient Boosting (e.g., LambdaMART), Neural Networks (e.g., RankNet).","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"What are good unsupervised baselines for text information retrieval?","answer":"Baselines: TF-IDF with cosine similarity, BM25, and Latent Semantic Analysis (LSA). These rank documents based on term relevance without labeled data.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"How would you evaluate your ranking algorithms? Which offline metrics would you use?","answer":"Evaluate using: Precision@k, Recall@k, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG). These assess ranking quality on labeled data.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"What is precision and recall at k?","answer":"Precision@k is the proportion of relevant items in the top k results. Recall@k is the proportion of relevant items retrieved in top k out of all relevant items.","difficulty":"Easy","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"What is mean average precision at k?","answer":"MAP@k averages the precision@k across multiple queries, considering only relevant items’ ranks, providing a comprehensive ranking quality metric.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"How can we use machine learning for search?","answer":"Use ML to rank documents by relevance, employing features like TF-IDF, embeddings, or user behavior, with models like RankSVM or neural networks.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"How can we get training data for our ranking algorithms?","answer":"Collect data via: 1) Explicit user feedback (ratings). 2) Implicit signals (clicks, dwell time). 3) Human annotations for relevance. 4) Synthetic data generation.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"Can we formulate the search problem as a classification problem? How?","answer":"Yes, treat search as binary classification (relevant/irrelevant) per query-document pair. Train models to predict relevance scores, then rank by scores.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"How can we use clicks data as the training data for ranking algorithms?","answer":"Use clicks as implicit feedback, treating clicked items as relevant. Model click probability or use pairwise ranking (e.g., clicked vs. non-clicked) to train models.","difficulty":"Hard","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"Do you know how to use gradient boosting trees for ranking?","answer":"Use Gradient Boosting for ranking with algorithms like LambdaMART, optimizing pairwise or listwise loss (e.g., NDCG). Features include query-document relevance and user signals.","difficulty":"Hard","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"How do you do an online evaluation of a new ranking algorithm?","answer":"Perform A/B testing, serving the new algorithm to a user subset and comparing metrics like click-through rate, engagement, or conversion against the baseline.","difficulty":"Medium","topic":"Ranking and Search","role":"Machine Learning Engineer"}
{"question":"What is a recommender system?","answer":"Recommender systems suggest items likely to interest users, using techniques like collaborative filtering or content-based filtering, e.g., product recommendations.","difficulty":"Easy","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"What are good baselines when building a recommender system?","answer":"Baselines: Popularity-based (recommend trending items), Content-based (item similarity), Collaborative Filtering (user-item interactions). Ensure relevance, diversity, and novelty.","difficulty":"Medium","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"What is collaborative filtering?","answer":"Collaborative filtering recommends items based on user-item interactions, averaging similar users’ preferences. It uses similarity metrics and normalization but struggles with sparsity.","difficulty":"Medium","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"How we can incorporate implicit feedback (clicks, etc) into our recommender systems?","answer":"Use implicit feedback (e.g., clicks) with weighted alternating least squares (wALS), modeling preference strength to predict user-item interactions, handling missing negatives.","difficulty":"Medium","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"What is the cold start problem?","answer":"Cold start occurs when new users or items lack interaction data, preventing collaborative filtering recommendations. It affects both user and item recommendations.","difficulty":"Medium","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"Possible approaches to solving the cold start problem?","answer":"Solutions: 1) Content-based filtering (use item features). 2) Demographic filtering (user profiles). 3) Hybrid methods combining both to leverage available data.","difficulty":"Medium","topic":"Recommender Systems","role":"Machine Learning Engineer"}
{"question":"What is a time series?","answer":"A time series is a sequence of data points ordered in time, typically collected at regular intervals, e.g., stock prices or temperature readings.","difficulty":"Easy","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"How is time series different from the usual regression problem?","answer":"Time series forecasting predicts future values based on temporal patterns, unlike regression, which uses independent features. Time series often involves trends and seasonality.","difficulty":"Easy","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"Which models do you know for solving time series problems?","answer":"Models: Simple Exponential Smoothing, Holt’s Method (trend), Holt-Winter’s (trend+seasonality), ARIMA, Time Series Decomposition, RNNs, LSTMs.","difficulty":"Medium","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"If there’s a trend in our series, how we can remove it? And why would we want to do it?","answer":"Remove trends via differencing or modeling (e.g., Holt’s Method) to achieve stationarity, required by many time series models for valid analysis.","difficulty":"Medium","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"You have a series with only one variable 'y' measured at time t. How do predict 'y' at time t+1? Which approaches would you use?","answer":"Use autoregressive models (e.g., ARIMA) to predict y_t+1 based on past y values, leveraging autocorrelation. Alternatively, use RNNs for complex patterns.","difficulty":"Medium","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"You have a series with a variable 'y' and a set of features. How do you predict 'y' at t+1? Which approaches would you use?","answer":"Use causal forecasting with linear/multiple regression or deep learning (e.g., LSTMs) if features are predictive. Ensure features are time-aligned.","difficulty":"Medium","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"What are the problems with using trees for solving time series problems?","answer":"Random Forests can’t extrapolate trends, predicting averages instead. They struggle with temporal dependencies, making them less suitable for time series.","difficulty":"Medium","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"What is a Support Vector Machine (SVM), and how does it work for classification tasks?","answer":"SVM is a supervised algorithm that finds the optimal hyperplane maximizing the margin between classes. It uses the kernel trick for non-linear data, effective for small/large datasets.","difficulty":"Medium","topic":"Support Vector Machines","role":"Machine Learning Engineer"}
{"question":"How would you handle missing data in a dataset before training a model?","answer":"Handle missing data by: 1) Removing rows/columns with minimal impact. 2) Imputing with mean/median or advanced methods (KNN). 3) Flagging missingness. 4) Using domain knowledge.","difficulty":"Medium","topic":"Data Preprocessing","role":"Machine Learning Engineer"}
{"question":"What is the difference between bagging and boosting?","answer":"Bagging trains independent models on random subsets, reducing variance (e.g., Random Forest). Boosting trains sequentially, correcting errors, reducing bias/variance (e.g., XGBoost).","difficulty":"Medium","topic":"Ensemble Methods","role":"Machine Learning Engineer"}
{"question":"What is batch normalization, and why is it used in neural networks?","answer":"Batch normalization normalizes layer inputs to mean 0, variance 1 per mini-batch, improving training speed, stability, and convergence, reducing gradient issues.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How would you design a real-time fraud detection system?","answer":"1) Collect transactional data. 2) Engineer features (e.g., frequency). 3) Use fast models (e.g., logistic regression). 4) Implement streaming (Kafka). 5) Tune thresholds. 6) Monitor and retrain.","difficulty":"Hard","topic":"System Design","role":"Machine Learning Engineer"}
{"question":"What is Naive Bayes, and when is it used?","answer":"Naive Bayes is a probabilistic classifier assuming feature independence, used for text classification (e.g., spam detection) due to simplicity and speed.","difficulty":"Medium","topic":"Naive Bayes","role":"Machine Learning Engineer"}
{"question":"What is an autoencoder, and what are its applications?","answer":"An autoencoder is a neural network that learns to reconstruct input data, used for dimensionality reduction, denoising, and anomaly detection.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How does the k-Nearest Neighbors (kNN) algorithm work?","answer":"kNN classifies a point based on the majority class of its k nearest neighbors, using distance metrics (e.g., Euclidean). It’s simple but computationally intensive.","difficulty":"Easy","topic":"k-Nearest Neighbors","role":"Machine Learning Engineer"}
{"question":"What is the difference between L1 and L2 loss functions?","answer":"L1 loss is the absolute difference between predicted and actual values, robust to outliers. L2 loss is the squared difference, sensitive to outliers but smoother.","difficulty":"Medium","topic":"Loss Functions","role":"Machine Learning Engineer"}
{"question":"What is cross-entropy loss, and when is it used?","answer":"Cross-entropy loss measures the difference between predicted and true probability distributions, commonly used in classification tasks (e.g., logistic regression, neural networks).","difficulty":"Medium","topic":"Loss Functions","role":"Machine Learning Engineer"}
{"question":"How would you implement feature scaling, and why is it important?","answer":"Implement scaling via standardization (mean 0, variance 1) or min-max scaling (0 to 1). It’s important for algorithms sensitive to feature scales (e.g., SVM, neural networks).","difficulty":"Medium","topic":"Data Preprocessing","role":"Machine Learning Engineer"}
{"question":"What is the role of a validation set in machine learning?","answer":"The validation set tunes hyperparameters and monitors model performance during training, preventing overfitting and ensuring generalization.","difficulty":"Easy","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"What is a ROC-AUC score, and how is it interpreted?","answer":"ROC-AUC measures a classifier’s ability to distinguish classes. AUC = 1 is perfect; 0.5 is random guessing; <0.5 is worse than random.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"How does the AdaBoost algorithm work?","answer":"AdaBoost combines weak learners (e.g., stumps) by weighting misclassified samples higher in each iteration, focusing on hard examples to improve accuracy.","difficulty":"Medium","topic":"Ensemble Methods","role":"Machine Learning Engineer"}
{"question":"What is the difference between supervised and unsupervised learning?","answer":"Supervised learning uses labeled data to predict outputs; unsupervised learning finds patterns in unlabeled data (e.g., clustering, dimensionality reduction).","difficulty":"Easy","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"What is a hyperparameter, and how is it different from a model parameter?","answer":"Hyperparameters are set before training (e.g., learning rate); model parameters are learned during training (e.g., weights). Hyperparameters control model behavior.","difficulty":"Easy","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"How would you handle class imbalance in a classification problem?","answer":"Handle imbalance by: 1) Resampling (over/under-sampling). 2) Using class weights. 3) Generating synthetic data (SMOTE). 4) Choosing metrics like F1-score.","difficulty":"Medium","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"What is the difference between Type I and Type II errors?","answer":"Type I error (false positive) incorrectly rejects a true null hypothesis. Type II error (false negative) fails to reject a false null hypothesis.","difficulty":"Medium","topic":"Statistics","role":"Machine Learning Engineer"}
{"question":"What is the purpose of early stopping in neural network training?","answer":"Early stopping halts training when validation performance stops improving, preventing overfitting and saving computation.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How does the Expectation-Maximization (EM) algorithm work?","answer":"EM iteratively estimates parameters for models with latent variables, alternating between Expectation (E-step: estimate latent variables) and Maximization (M-step: optimize parameters).","difficulty":"Hard","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"What is a Gaussian Mixture Model (GMM)?","answer":"GMM is a probabilistic model representing data as a mixture of Gaussian distributions, used for clustering and density estimation, often fit via EM.","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"How would you optimize a model for low-latency inference in production?","answer":"Optimize by: 1) Pruning/quantizing models. 2) Using lightweight architectures (e.g., MobileNet). 3) Batching inputs. 4) Leveraging hardware accelerators (GPUs/TPUs). 5) Caching frequent predictions.","difficulty":"Hard","topic":"Model Deployment","role":"Machine Learning Engineer"}
{"question":"What is the difference between online and batch learning?","answer":"Online learning updates the model incrementally with new data; batch learning trains on the entire dataset at once. Online is suitable for streaming data.","difficulty":"Medium","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"What is a confusion matrix, and how is it used?","answer":"A confusion matrix summarizes classification performance with TP, TN, FP, FN, enabling calculation of metrics like accuracy, precision, and recall.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"How does the Random Forest algorithm handle missing values?","answer":"Random Forests handle missing values by imputing them (e.g., median for numerical, mode for categorical) or using surrogate splits based on other features.","difficulty":"Medium","topic":"Random Forest","role":"Machine Learning Engineer"}
{"question":"What is the role of the kernel trick in SVM?","answer":"The kernel trick maps data into a higher-dimensional space to make it linearly separable without explicitly computing the transformation, improving SVM efficiency.","difficulty":"Medium","topic":"Support Vector Machines","role":"Machine Learning Engineer"}
{"question":"How would you detect and handle outliers in a dataset?","answer":"Detect outliers using statistical methods (e.g., Z-score, IQR) or visualization (boxplots). Handle by removing, capping, or transforming them, depending on the task.","difficulty":"Medium","topic":"Data Preprocessing","role":"Machine Learning Engineer"}
{"question":"What is the difference between hard and soft margin SVM?","answer":"Hard margin SVM requires perfect class separation, failing with noise. Soft margin SVM allows some misclassifications, introducing a slack variable for robustness.","difficulty":"Medium","topic":"Support Vector Machines","role":"Machine Learning Engineer"}
{"question":"What is the purpose of a learning curve in machine learning?","answer":"Learning curves plot training/validation performance vs. dataset size, diagnosing bias (underfitting) or variance (overfitting) to guide model improvements.","difficulty":"Medium","topic":"Model Evaluation","role":"Machine Learning Engineer"}
{"question":"How does the LSTM architecture work for sequence modeling?","answer":"LSTM (Long Short-Term Memory) uses gates (input, forget, output) to selectively remember or forget information, handling long-term dependencies in sequences.","difficulty":"Hard","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is the vanishing gradient problem, and how is it addressed?","answer":"Vanishing gradients occur when gradients become too small during backpropagation, slowing learning. Address with ReLU, LSTMs, or gradient clipping.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is a Transformer model, and how does it differ from RNNs?","answer":"Transformers use self-attention to process sequences in parallel, unlike sequential RNNs. They’re faster, handle long dependencies better, and are used in NLP (e.g., BERT).","difficulty":"Hard","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How would you implement a function to calculate precision and recall in Python?","answer":"```python\ndef precision_recall(y_true, y_pred):\n    tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n    fp = sum((t == 0 and p == 1) for t, p in zip(y_true, y_pred))\n    fn = sum((t == 1 and p == 0) for t, p in zip(y_true, y_pred))\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    return precision, recall\n``` This computes precision (TP/(TP+FP)) and recall (TP/(TP+FN)).","difficulty":"Medium","topic":"Coding","role":"Machine Learning Engineer"}
{"question":"What is the difference between precision and accuracy?","answer":"Precision is the proportion of true positive predictions among positive predictions. Accuracy is the proportion of correct predictions overall. Precision focuses on positive class accuracy.","difficulty":"Easy","topic":"Classification","role":"Machine Learning Engineer"}
{"question":"How does the k-fold cross-validation prevent overfitting?","answer":"K-fold cross-validation uses multiple train/validation splits, averaging performance to reduce reliance on a single split, providing a robust estimate and mitigating overfitting.","difficulty":"Medium","topic":"Validation","role":"Machine Learning Engineer"}
{"question":"What is the role of feature engineering in machine learning?","answer":"Feature engineering creates or transforms features to improve model performance, capturing domain knowledge and making patterns more learnable.","difficulty":"Easy","topic":"Feature Engineering","role":"Machine Learning Engineer"}
{"question":"How would you select features for a machine learning model?","answer":"Select features using: 1) Filter methods (e.g., correlation). 2) Wrapper methods (e.g., recursive feature elimination). 3) Embedded methods (e.g., Lasso). Evaluate via cross-validation.","difficulty":"Medium","topic":"Feature Selection","role":"Machine Learning Engineer"}
{"question":"What is the difference between a generative and discriminative model?","answer":"Generative models (e.g., Naive Bayes) model joint probability P(X,Y), generating data. Discriminative models (e.g., logistic regression) model conditional P(Y|X), focusing on classification.","difficulty":"Medium","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"How does the XGBoost algorithm improve over standard gradient boosting?","answer":"XGBoost adds regularization, parallel processing, tree pruning, and handling missing values, improving speed, scalability, and accuracy over standard gradient boosting.","difficulty":"Medium","topic":"Gradient Boosting","role":"Machine Learning Engineer"}
{"question":"What is the purpose of a loss function in machine learning?","answer":"A loss function quantifies the error between predicted and actual values, guiding model optimization to minimize errors during training.","difficulty":"Easy","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"How would you deploy a machine learning model in production?","answer":"1) Train and validate the model. 2) Save model artifacts (e.g., pickle). 3) Containerize (Docker). 4) Deploy on a platform (e.g., AWS SageMaker). 5) Monitor performance and retrain.","difficulty":"Hard","topic":"Model Deployment","role":"Machine Learning Engineer"}
{"question":"What is the difference between a decision tree and a random forest?","answer":"A decision tree is a single model splitting data based on features. Random Forest is an ensemble of trees, using bagging and feature randomness to reduce variance.","difficulty":"Easy","topic":"Ensemble Methods","role":"Machine Learning Engineer"}
{"question":"How does the attention mechanism work in Transformers?","answer":"Attention assigns weights to input tokens based on their relevance to each other, allowing the model to focus on important parts of the sequence, improving context understanding.","difficulty":"Hard","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"What is the difference between regression and classification?","answer":"Regression predicts continuous outputs (e.g., price); classification predicts discrete classes (e.g., spam/not spam). Both are supervised learning tasks.","difficulty":"Easy","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"How would you handle multicollinearity in a regression model?","answer":"Handle multicollinearity by: 1) Removing correlated features. 2) Using PCA for dimensionality reduction. 3) Applying L2 regularization (Ridge). 4) Using VIF to identify problematic features.","difficulty":"Medium","topic":"Regression","role":"Machine Learning Engineer"}
{"question":"What is the role of the softmax function in neural networks?","answer":"Softmax converts raw scores (logits) into probabilities, summing to 1, used in multi-class classification to assign class probabilities.","difficulty":"Medium","topic":"Neural Networks","role":"Machine Learning Engineer"}
{"question":"How does the t-SNE algorithm work for dimensionality reduction?","answer":"t-SNE minimizes the divergence between probability distributions of high-dimensional and low-dimensional data, preserving local structure for visualization.","difficulty":"Hard","topic":"Dimensionality Reduction","role":"Machine Learning Engineer"}
{"question":"What is the difference between overfitting and underfitting?","answer":"Overfitting occurs when a model learns noise, performing well on training but poorly on test data. Underfitting fails to learn patterns, performing poorly on both.","difficulty":"Easy","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}
{"question":"How would you evaluate a clustering algorithm?","answer":"Evaluate clustering with: 1) Silhouette score (cohesion/separation). 2) Davies-Bouldin index (cluster similarity). 3) Visual inspection (e.g., scatter plots). 4) Domain-specific metrics.","difficulty":"Medium","topic":"Clustering","role":"Machine Learning Engineer"}
{"question":"What is the difference between PCA and LDA?","answer":"PCA (unsupervised) reduces dimensionality by maximizing variance. LDA (supervised) maximizes class separability, used when class labels are available.","difficulty":"Medium","topic":"Dimensionality Reduction","role":"Machine Learning Engineer"}
{"question":"How does the ARIMA model work for time series forecasting?","answer":"ARIMA combines AutoRegressive (AR), Integrated (I, differencing for stationarity), and Moving Average (MA) components to model and forecast time series data.","difficulty":"Hard","topic":"Time Series","role":"Machine Learning Engineer"}
{"question":"What is the purpose of data normalization in machine learning?","answer":"Normalization scales features to a common range (e.g., 0-1), ensuring equal contribution to models and improving convergence for algorithms like SVM or neural networks.","difficulty":"Easy","topic":"Data Preprocessing","role":"Machine Learning Engineer"}
{"question":"How would you explain a machine learning model to a non-technical stakeholder?","answer":"Explain the model as a tool that learns patterns from data to make predictions, like recommending products based on past purchases, emphasizing benefits and outcomes.","difficulty":"Medium","topic":"Communication","role":"Machine Learning Engineer"}
{"question":"What is the difference between a parametric and non-parametric model?","answer":"Parametric models (e.g., linear regression) assume a fixed form with parameters. Non-parametric models (e.g., kNN) have flexible forms, adapting to data complexity.","difficulty":"Medium","topic":"Machine Learning Basics","role":"Machine Learning Engineer"}